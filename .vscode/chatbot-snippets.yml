# java-langchain4j section

snippet langchain4j-dep:
  name: "LangChain4j dependency"
  prefix: "java-01"
  scope: "xml"
  body: |
    <dependency>
      <groupId>dev.langchain4j</groupId>
      <artifactId>langchain4j</artifactId>
      <version>${langchain4j.version}</version>
    </dependency>

    <dependency>
      <groupId>dev.langchain4j</groupId>
      <artifactId>langchain4j-mistral-ai</artifactId>
      <version>${langchain4j.provider.version}</version>
    </dependency>

snippet langchain4j-ovhai:
  name: "LangChain4j OVHai dependency"
  prefix: "java-15"
  scope: "xml"
  body: |
    <dependency>
      <groupId>dev.langchain4j</groupId>
      <artifactId>langchain4j-ovh-ai</artifactId>
      <version>${langchain4j.provider.version}</version>
    </dependency>

snippet simple-ai-services:
  name: "Simple chatbot AI Services"
  prefix: "java-02"
  scope: "java"
  body: |
    interface Assistant {
      @SystemMessage("You are Nestor, a virtual assistant. Answer to the question.")
      String chat(String message);
    }

snippet simple-model:
  name: "Simple chatbot model"
  prefix: "java-03"
  scope: "java"
  body: |
    MistralAiChatModel chatModel = MistralAiChatModel.builder()
        .apiKey(System.getenv("OVH_AI_ENDPOINTS_ACCESS_TOKEN"))
        .modelName(System.getenv("OVH_AI_ENDPOINTS_MODEL_NAME"))
        .baseUrl(System.getenv("OVH_AI_ENDPOINTS_MODEL_URL"))
        .maxTokens(512)
        .temperature(0.0)
        .logRequests(true)
        .logResponses(true)
        .build();

snippet simple-assistant:
  name: "Simple chatbot assistant"
  prefix: "java-04"
  scope: "java"
  body: |
    Assistant assistant = AiServices.builder(Assistant.class)
        .chatModel(chatModel)
        .build();

snippet simple-call:
  name: "Simple chatbot call"
  prefix: "java-05"
  scope: "java"
  body: |
    _LOG.info("ðŸ’¬: Question: Tell me a joke about Java developers\n");
    _LOG.info("ðŸ¤–: {}", assistant.chat("Tell me a joke about Java developers"));

snippet streaming-ai-services:
  name: "Streaming AI Services"
  prefix: "java-06"
  scope: "java"
  body: |
    interface Assistant {
      @SystemMessage("You are Nestor, a virtual assistant. Answer to the question.")
      TokenStream chat(String message);
    }

snippet streaming-model:
  name: "Streaming model"
  prefix: "java-07"
  scope: "java"
  body: |
    MistralAiStreamingChatModel steamingModel = MistralAiStreamingChatModel.builder()
        .apiKey(System.getenv("OVH_AI_ENDPOINTS_ACCESS_TOKEN"))
        .modelName(System.getenv("OVH_AI_ENDPOINTS_MODEL_NAME"))
        .baseUrl(System.getenv("OVH_AI_ENDPOINTS_MODEL_URL"))
        .maxTokens(512)
        .temperature(0.0)
        .logRequests(false)
        .logResponses(false)
        .build();

snippet streaming-assistant:
  name: "Streaming assistant"
  prefix: "java-08"
  scope: "java"
  body: |
    Assistant assistant = AiServices.builder(Assistant.class)
        .streamingChatModel(steamingModel)
        .build();

snippet streaming-call:
  name: "Streaming call"
  prefix: "java-09"
  scope: "java"
  body: |
    _LOG.info("ðŸ’¬: Tell me a joke about Java developers\n");
    TokenStream tokenStream = assistant.chat("Tell me a joke about Java developers");
    _LOG.info("ðŸ¤–: ");
    tokenStream
        .onPartialResponse(_LOG::info)
        .onError(Throwable::printStackTrace).start();

snippet memory-ai-services:
  name: "Memory call"
  prefix: "java-10"
  scope: "java"
  body: |
    interface Assistant {
      @SystemMessage("You are Nestor, a virtual assistant. Answer to the question.")
      TokenStream chat(String message);
    }

snippet memory-model:
  name: "Memory model"
  prefix: "java-11"
  scope: "java"
  body: |
    MistralAiStreamingChatModel steamingModel = MistralAiStreamingChatModel.builder()
        .apiKey(System.getenv("OVH_AI_ENDPOINTS_ACCESS_TOKEN"))
        .modelName(System.getenv("OVH_AI_ENDPOINTS_MODEL_NAME"))
        .baseUrl(System.getenv("OVH_AI_ENDPOINTS_MODEL_URL"))
        .maxTokens(512)
        .temperature(0.0)
        .logRequests(false)
        .logResponses(false)
        .build();

snippet memory-memory:
  name: "Memory memory storage"
  prefix: "java-12"
  scope: "java"
  body: |
    ChatMemory chatMemory = MessageWindowChatMemory.withMaxMessages(10);

snippet memory-assistant:
  name: "Memory memory assistant"
  prefix: "java-13"
  scope: "java"
  body: |
    Assistant assistant = AiServices.builder(Assistant.class)
        .streamingChatModel(steamingModel)
        .chatMemory(chatMemory)
        .build();  

snippet memory-call:
  name: "Memory memory call"
  prefix: "java-14"
  scope: "java"
  body: |
    _LOG.info("ðŸ’¬: My name is StÃ©phane.\n");
        TokenStream tokenStream = assistant.chat("My name is StÃ©phane.");
        _LOG.info("ðŸ¤–: ");
        tokenStream
                .onPartialResponse(_LOG::info)
                .onCompleteResponse(token -> {
                    _LOG.info("\nðŸ’¬: Do you remember what is my name?\n");
                    _LOG.info("ðŸ¤–: ");
                    assistant.chat("Do you remember what is my name?")
                            .onPartialResponse(_LOG::info)
                            .onError(Throwable::printStackTrace).start();
                })
                .onError(Throwable::printStackTrace).start();

snippet rag-ai-services:
  name: "RAG AI Services"
  prefix: "java-16"
  scope: "java"
  body: |
    interface Assistant {
      @SystemMessage("You are Nestor, a virtual assistant. Answer to the question.")
      TokenStream chat(String message);
    }

snippet rag-model:
  name: "RAG model"
  prefix: "java-17"
  scope: "java"
  body: |
    MistralAiStreamingChatModel steamingModel = MistralAiStreamingChatModel.builder()
            .apiKey(System.getenv("OVH_AI_ENDPOINTS_ACCESS_TOKEN"))
            .modelName(System.getenv("OVH_AI_ENDPOINTS_MODEL_NAME"))
            .baseUrl(System.getenv("OVH_AI_ENDPOINTS_MODEL_URL"))
            .maxTokens(512)
            .temperature(0.0)
            .logRequests(false)
            .logResponses(false)
            .build();

snippet rag-memory:
  name: "RAG memory"
  prefix: "java-18"
  scope: "java"
  body: |
    ChatMemory chatMemory = MessageWindowChatMemory.withMaxMessages(10);

snippet rag-chunk:
  name: "RAG chunk"
  prefix: "java-19"
  scope: "java"
  body: |
    DocumentParser documentParser = new TextDocumentParser();
    Document document = loadDocument(
        RAGChatbot.class.getResource("/rag-files/conference-information-talk-01.md").getFile(),
        documentParser);
    DocumentSplitter splitter = DocumentSplitters.recursive(8000, 50);

    List<TextSegment> segments = splitter.split(document);

snippet rag-embedding:
  name: "RAG embedding"
  prefix: "java-20"
  scope: "java"
  body: |
    EmbeddingModel embeddingModel = OvhAiEmbeddingModel.builder()
                    .apiKey(System.getenv("OVH_AI_ENDPOINTS_ACCESS_TOKEN"))
                    .baseUrl(System.getenv("OVH_AI_ENDPOINTS_EMBEDDING_MODEL"))
                    .build();
    List<Embedding> embeddings = embeddingModel.embedAll(segments).content();

snippet rag-embedding-store:
  name: "RAG embedding store"
  prefix: "java-21"
  scope: "java"
  body: |
    EmbeddingStore<TextSegment> embeddingStore = new InMemoryEmbeddingStore<>();
    embeddingStore.addAll(embeddings, segments);
    ContentRetriever contentRetriever = EmbeddingStoreContentRetriever.builder()
        .embeddingStore(embeddingStore)
        .embeddingModel(embeddingModel)
        .maxResults(3)
        .minScore(0.1)
        .build();

snippet rag-assistant:
  name: "RAG assistant"
  prefix: "java-22"
  scope: "java"
  body: |
    Assistant assistant = AiServices.builder(Assistant.class)
        .streamingChatModel(steamingModel)
        .chatMemory(chatMemory)
        .contentRetriever(contentRetriever)
        .build();

snippet rag-call:
  name: "RAG call"
  prefix: "java-23"
  scope: "java"
  body: |
    _LOG.info("ðŸ’¬: What is the program at Sunny Tech?\n");
    TokenStream tokenStream = assistant.chat("What is the program at Sunny Tech?");
    _LOG.info("ðŸ¤–: ");
    tokenStream
        .onPartialResponse(_LOG::info)
        .onError(Throwable::printStackTrace).start();

snippet java-24:
  name: "java-24-Tool"
  prefix: "java-24"
  scope: "java"
  body: |
    // Define the tool using the @Tool annotation
    @Tool("""
                Tool to create an image with Stable Diffusion XL given a prompt and a negative prompt.
                """)
    void generateImage(@P("Prompt that explains the image") String prompt, @P("Negative prompt that explains what the image must not contains") String negativePrompt) throws IOException, InterruptedException {
        _LOG.info("Prompt: {}", prompt);
        _LOG.info("Negative prompt: {}", negativePrompt);

        // java-25

        // java-26
    }

snippet java-25:
  name: "java-25-sdxl-call"
  prefix: "java-25"
  scope: "java"
  body: |
    // Call Stable diffusion model with the prompt and negative prompt
    HttpRequest httpRequest = HttpRequest.newBuilder()
            .uri(URI.create(System.getenv("OVH_AI_ENDPOINTS_SD_URL")))
            .POST(HttpRequest.BodyPublishers.ofString("""
                        {"prompt": "%s", 
                          "negative_prompt": "%s"}
                        """.formatted(prompt, negativePrompt)))
            .header("accept", "application/octet-stream")
            .header("Content-Type", "application/json")
            .header("Authorization", "Bearer " + System.getenv("OVH_AI_ENDPOINTS_ACCESS_TOKEN"))
            .build();

snippet java-26:
  name: "java-26-write-img"
  prefix: "java-26"
  scope: "java"
  body: |
    // Create the image file from Stable Diffusion response
    HttpResponse<byte[]> response = HttpClient.newHttpClient()
            .send(httpRequest, HttpResponse.BodyHandlers.ofByteArray());

    _LOG.debug("SDXL status code: {}", response.statusCode());
    Files.write(Path.of("generated-image.jpeg"), response.body());

snippet java-27:
  name: "java-27-ai-services"
  prefix: "java-27"
  scope: "java"
  body: |
    // Create a detailed system prompt: the goal and what the model must generate and use
    @SystemMessage("""
            Your are an expert of using the Stable Diffusion XL model.
            The user explains in natural language what kind of image he wants.
            You must do the following steps:
              - Understand the user's request.
              - Generate the two kinds of prompts for stable diffusion: the prompt and the negative prompt
              - the prompts must be in english and detailed and optimized for the Stable Diffusion XL model. 
              - once and only once you have this two prompts call the tool with the two prompts.
            If asked about to create an image, you MUST call the `generateImage` function.
            """)
    @UserMessage("Create an image with stable diffusion XL following this description: {{userMessage}}")
    String chat(@V("userMessage") String userMessage);

snippet java-28:
  name: "java-28-model"
  prefix: "java-28"
  scope: "java"
  body: |
    // Main chatbot configuration, try to be more deterministic as possible ;)
    MistralAiChatModel chatModel = MistralAiChatModel.builder()
            .apiKey(System.getenv("OVH_AI_ENDPOINTS_ACCESS_TOKEN"))
            .baseUrl(System.getenv("OVH_AI_ENDPOINTS_MODEL_URL"))
            .modelName(System.getenv("OVH_AI_ENDPOINTS_MODEL_NAME"))
            .logRequests(false)
            .logResponses(false)
            // To have more deterministic outputs, set temperature to 0.
            .temperature(0.0)
            .build();

snippet java-29:
  name: "java-29-model"
  prefix: "java-29"
  scope: "java"
  body: |
    // Add memory to fine tune the SDXL prompt.
    ChatMemory chatMemory = MessageWindowChatMemory.withMaxMessages(10);

snippet java-30:
  name: "java-30-ai-services"
  prefix: "java-30"
  scope: "java"
  body: |
    // Build the chatbot thanks to LangChain4J AI Services mode (see https://docs.langchain4j.dev/tutorials/ai-services)
    ChatBot chatBot = AiServices.builder(ChatBot.class)
            .chatModel(chatModel)
            .tools(new ImageGenTools())
            .chatMemory(chatMemory)
            .build();

snippet java-31:
  name: "java-31-prompt"
  prefix: "java-31"
  scope: "java"
  body: |
    // Start the conversation loop (enter "exit" to quit)
    String userInput = "";
    Scanner scanner = new Scanner(System.in);
    while (true) {
        _LOG.info("\nEnter your message: ");
        userInput = scanner.nextLine();
        if (userInput.equalsIgnoreCase("exit")) break;
        _LOG.info("\nResponse: " + chatBot.chat(userInput));
    }
    scanner.close();

snippet java-32:
  name: "java-32-dep-mcp"
  prefix: "java-32"
  scope: "xml"
  body: |
    <dependency>
      <groupId>dev.langchain4j</groupId>
      <artifactId>langchain4j-mcp</artifactId>
      <version>${langchain4j.provider.version}</version>
    </dependency>

snippet java-33:
  name: "java-33-ai-services"
  prefix: "java-33"
  scope: "java"
  body: |
    @SystemMessage("""
            Your are an expert of using the Stable Diffusion XL model.
            The user explains in natural language what kind of image he wants.
            You must do the following steps:
              - Understand the user's request.
              - Generate the two kinds of prompts for stable diffusion: the prompt and the negative prompt
              - the prompts must be in english and detailed and optimized for the Stable Diffusion XL model. 
              - once and only once you have this two prompts call the tool with the two prompts.
            If asked about to create an image, you MUST call the `generateImage` function.
            """)
    @UserMessage("Create an image with stable diffusion XL following this description: {{userMessage}}")
    String chat(@V("userMessage") String userMessage);

snippet java-34:
  name: "java-34-model"
  prefix: "java-34"
  scope: "java"
  body: |
    MistralAiChatModel chatModel = MistralAiChatModel.builder()
            .apiKey(System.getenv("OVH_AI_ENDPOINTS_ACCESS_TOKEN"))
            .baseUrl(System.getenv("OVH_AI_ENDPOINTS_MODEL_URL"))
            .modelName(System.getenv("OVH_AI_ENDPOINTS_MODEL_NAME"))
            .logRequests(false)
            .logResponses(false)
            // To have more deterministic outputs, set temperature to 0.
            .temperature(0.0)
            .build();

snippet java-35:
  name: "java-35-"
  prefix: "java-35-mcp-transport"
  scope: "java"
  body: |
    McpTransport transport = new HttpMcpTransport.Builder()
            // https://xxxx/mcp/sse
            .sseUrl(System.getenv("MCP_SERVER_URL"))
            .logRequests(false)
            .logResponses(false)
            .build();

snippet java-36:
  name: "java-36"
  prefix: "java-36-mcp-client"
  scope: "java"
  body: |
    McpClient mcpClient = new DefaultMcpClient.Builder()
            .transport(transport)
            .build();

snippet java-37:
  name: "java-37"
  prefix: "java-37-mcp-tool"
  scope: "java"
  body: |
    McpToolProvider toolProvider = McpToolProvider.builder()
            .mcpClients(mcpClient)
            .build();

snippet java-38:
  name: "java-38"
  prefix: "java-38-memory"
  scope: "java"
  body: |
    ChatMemory chatMemory = MessageWindowChatMemory.withMaxMessages(10);

snippet java-39:
  name: "java-39"
  prefix: "java-39-chatbot"
  scope: "java"
  body: |
    ChatBot bot = AiServices.builder(ChatBot.class)
            .chatModel(chatModel)
            .toolProvider(toolProvider)
            .chatMemory(chatMemory)
            .build();

snippet java-40:
  name: "java-40"
  prefix: "java-40-prompt"
  scope: "java"
  body: |
    String userInput = "";
    Scanner scanner = new Scanner(System.in);
    while (true) {
        _LOG.info("\nEnter your message: ");
        userInput = scanner.nextLine();
        if (userInput.equalsIgnoreCase("exit")) break;
        _LOG.info("\nResponse: " + bot.chat(userInput));
    }
    scanner.close();
    
## java-quarkus section
snippet quarkus-props:
  name: "Quarkus properties"
  prefix: "quarkus-01"
  body: |
    quarkus.langchain4j.mistralai.base-url=\${OVH_AI_ENDPOINTS_MODEL_URL}
    quarkus.langchain4j.mistralai.api-key=\${OVH_AI_ENDPOINTS_ACCESS_TOKEN}
    quarkus.langchain4j.mistralai.chat-model.max-tokens=512
    quarkus.langchain4j.mistralai.chat-model.model-name=\${OVH_AI_ENDPOINTS_MODEL_NAME}
    quarkus.langchain4j.mistralai.log-requests=true
    quarkus.langchain4j.mistralai.log-responses=false
    quarkus.langchain4j.mistralai.chat-model.temperature=0.2

snippet quarkus-svc-simple-class-annot:
  name: "Class annotation simple"
  prefix: "quarkus-02"
  scope: "java"
  body: |
    @RegisterAiService

snippet quarkus-svc-simple-question:
  name: "Entry point to llm simple"
  prefix: "quarkus-03"
  scope: "java"
  body: |
    @SystemMessage("You are a virtual assistant and your name is Nestor.")
    @UserMessage("Answer as best possible to the following question: {question}. The answer must be in a style of a virtual assistant.")
    String askAQuestion(String question);

snippet quarkus-simple-resource-annot:
  name: "Class annotation simple resource"
  prefix: "quarkus-04"
  scope: "java"
  body: |
    @Path("/chatbot")

snippet quarkus-simple-resource-inject:
  name: "Injection simple resource"
  prefix: "quarkus-05"
  scope: "java"
  body: |
    @Inject
    AISimpleService aiEndpointService;

snippet quarkus-simple-resource-ask:
  name: "Simple resource ask method"
  prefix: "quarkus-06"
  scope: "java"
  body: |
    @Path("simple")
    @POST
    public String ask(String question) {
      // Call the askAQuestion method of the AISimpleService service
      return aiEndpointService.askAQuestion(question);
    }
    
snippet quarkus-svc-advanced-class-annot:
  name: "Class annotation advanced" 
  prefix: "quarkus-07"
  scope: "java"
  body: |
    @RegisterAiService

snippet quarkus-svc-advanced-question:
  name: "Entry point to llm advanced"
  prefix: "quarkus-08"
  scope: "java"
  body: |
    @SystemMessage("You are a virtual assistant and your name is Nestor.")
    @UserMessage("Answer as best possible to the following question: {question}. The answer must be in a style of a virtual assistant.")
    Multi<String> askAQuestion(String question);

snippet quarkus-advanced-resource-annot:
  name: "Class annotation advanced resource"
  prefix: "quarkus-09"
  scope: "java"
  body: |
    @Path("/chatbot")

snippet quarkus-advanced-resource-inject:
  name: "Injection advanced resource"
  prefix: "quarkus-10"
  scope: "java"
  body: |
    @Inject
    AIAdvancedService advancedService;

snippet quarkus-advanced-resource-ask:
  name: "Advanced resource ask method"
  prefix: "quarkus-11"
  scope: "java"
  body: |
    @Path("advanced")
    @POST
    public Multi<String> ask(String question) {
      // Call the askAQuestion method of the AISimpleService service and stream the
      // answer, see https://quarkus.io/guides/getting-started-reactive
      return advancedService.askAQuestion(question);
    }

snippet quarkus-svc-memory-class-annot:
  name: "Class annotation memory"
  prefix: "quarkus-12"
  scope: "java"
  body: |
    @ApplicationScoped
    @RegisterAiService

snippet quarkus-svc-memory-question:
  name: "Entry point to llm memory"
  prefix: "quarkus-13"
  scope: "java"
  body: |
    @SystemMessage("You are a virtual assistant and your name is Nestor.")
    @UserMessage("Answer as best possible to the following question: {question}. The answer must be in a style of a virtual assistant.")
    Multi<String> askAQuestion(String question, @MemoryId String memoryId);

snippet quarkus-memory-resource-annot:
  name: "Class annotation memory resource"
  prefix: "quarkus-14"
  scope: "java"
  body: |
    @Path("/chatbot")

snippet quarkus-memory-resource-inject:
  name: "Injection memory resource"
  prefix: "quarkus-15"
  scope: "java"
  body: |
    @Inject
    AIMemoryService aiMemoryService;

snippet quarkus-memory-resource-ask:
  name: "Memory resource ask method"
  prefix: "quarkus-16"
  scope: "java"
  body: |
    @Path("memory")
    @POST
    public Multi<String> ask(String question) {
      // Call the askAQuestion method of the AISimpleService service and stream the
      // answer, see https://quarkus.io/guides/getting-started-reactive
      return aiMemoryService.askAQuestion(question, "user-one");
    }

snippet quarkus-17:
  name: "quarkus-17-dep-mcp"
  prefix: "quarkus-17"
  scope: "xml"
  body: |
    <dependency>
        <groupId>io.quarkiverse.mcp</groupId>
        <artifactId>quarkus-mcp-server-sse</artifactId>
        <version>1.2.1</version>
    </dependency>

snippet quarkus-18:
  name: "quarkus-18-prop-mcp"
  prefix: "quarkus-18"
  body: |
    # MCP configuration
    quarkus.rest-client."com.ovhcloud.ai.quarkus.chatbot.service.StableDiffusionService".url=\${OVH_AI_ENDPOINTS_SD_URL}

snippet quarkus-19:
  name: "quarkus-19-svc-sdxl"
  prefix: "quarkus-19"
  scope: "java"
  body: |
    @RegisterRestClient
    @ClientHeaderParam(name = "Content-Type", value = "application/json")

snippet quarkus-20:
  name: "quarkus-20-call-sdxl"
  prefix: "quarkus-20"
  scope: "java"
  body: |
    @POST
    @ClientHeaderParam(name = "Authorization", value = "Bearer ${quarkus.langchain4j.mistralai.api-key}")
    byte[] generateImage(SDPayload payload);

snippet quarkus-21:
  name: "quarkus-21-inject-sdxl"
  prefix: "quarkus-21"
  scope: "java"
  body: |
    @RestClient
    StableDiffusionService stableDiffusionService;

snippet quarkus-22:
  name: "quarkus-22-tool"
  prefix: "quarkus-22"
  scope: "java"
  body: |
    // Define the tool using the @Tool annotation
    @Tool(description = "Tool to create an image with Stable Diffusion XL given a prompt and a negative prompt.")
    String generateImage(@P("Prompt that explains the image") String prompt, @P("Negative prompt that explains what the image must not contains") String negativePrompt) throws IOException, InterruptedException {
        _LOG.info("Prompt: {}", prompt);
        _LOG.info("Negative prompt: {}", negativePrompt);

        // quarkus-23
        return "Image generated";
    }

snippet quarkus-23:
  name: "quarkus-23-gen-img"
  prefix: "quarkus-23"
  scope: "java"
  body: |
    byte[] image = stableDiffusionService.generateImage(new SDPayload(prompt, negativePrompt));

    Files.write(Path.of("generated-image.jpeg"), image);
