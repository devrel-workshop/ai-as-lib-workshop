# java-langchain4j section

snippet java-01:
  name: "java LangChain4J dependencies"
  prefix: "java-01"
  scope: "xml"
  body: |
    <dependency>
      <groupId>dev.langchain4j</groupId>
      <artifactId>langchain4j</artifactId>
      <version>${langchain4j.version}</version>
    </dependency>

    <dependency>
      <groupId>dev.langchain4j</groupId>
      <artifactId>langchain4j-mistral-ai</artifactId>
      <version>${langchain4j.version}</version>
    </dependency>

snippet java-02:
  name: "Simple chatbot AI Services"
  prefix: "java-02"
  scope: "java"
  body: |
    interface Assistant {
      @SystemMessage("You are Nestor, a virtual assistant. Answer to the question.")
      String chat(String message);
    }

snippet java-03:
  name: "Simple chatbot model"
  prefix: "java-03"
  scope: "java"
  body: |
    ChatModel chatModel = OpenAiChatModel.builder()
      .apiKey(System.getenv("OVH_AI_ENDPOINTS_ACCESS_TOKEN"))
      .modelName(System.getenv("OVH_AI_ENDPOINTS_MODEL_NAME"))
      .baseUrl(System.getenv("OVH_AI_ENDPOINTS_MODEL_URL"))
      .maxTokens(512)
      .temperature(0.0)
      .logRequests(false)
      .logResponses(false)
    .build();

snippet java-04:
  name: "Simple chatbot assistant"
  prefix: "java-04"
  scope: "java"
  body: |
    Assistant assistant = AiServices.builder(Assistant.class)
        .chatModel(chatModel)
        .build();

snippet java-05:
  name: "Simple chatbot call"
  prefix: "java-05"
  scope: "java"
  body: |
    _LOG.info("üí¨: Question: Tell me a joke about Java developers\n");
    _LOG.info("ü§ñ: {}", assistant.chat("Tell me a joke about Java developers"));

snippet java-06:
  name: "Streaming AI Services"
  prefix: "java-06"
  scope: "java"
  body: |
    interface Assistant {
      @SystemMessage("You are Nestor, a virtual assistant. Answer to the question.")
      TokenStream chat(String message);
    }

snippet java-07:
  name: "Streaming model"
  prefix: "java-07"
  scope: "java"
  body: |
    StreamingChatModel steamingModel = OpenAiStreamingChatModel.builder()
            .apiKey(System.getenv("OVH_AI_ENDPOINTS_ACCESS_TOKEN"))
            .modelName(System.getenv("OVH_AI_ENDPOINTS_MODEL_NAME"))
            .baseUrl(System.getenv("OVH_AI_ENDPOINTS_MODEL_URL"))
            .maxTokens(512)
            .temperature(0.0)
            .logRequests(false)
            .logResponses(false)
            .build();

snippet streaming-assistant:
  name: "Streaming assistant"
  prefix: "java-08"
  scope: "java"
  body: |
    Assistant assistant = AiServices.builder(Assistant.class)
        .streamingChatModel(steamingModel)
        .build();

snippet java-09:
  name: "Streaming call"
  prefix: "java-09"
  scope: "java"
  body: |
    _LOG.info("üí¨: Tell me a joke about Java developers\n");
    TokenStream tokenStream = assistant.chat("Tell me a joke about Java developers");
    CompletableFuture<ChatResponse> futureChatResponse = new CompletableFuture<>();

    _LOG.info("ü§ñ: ");
    tokenStream
        .onCompleteResponse(response -> futureChatResponse.complete(response))
        .onPartialResponse(_LOG::info)
        .onError(Throwable::printStackTrace).start();
    futureChatResponse.join();

snippet java-10:
  name: "Memory AI Services"
  prefix: "java-10"
  scope: "java"
  body: |
    interface Assistant {
      @SystemMessage("You are Nestor, a virtual assistant. Answer to the question.")
      TokenStream chat(String message);
    }

snippet java-11:
  name: "Memory model"
  prefix: "java-11"
  scope: "java"
  body: |
    StreamingChatModel steamingModel = OpenAiStreamingChatModel.builder()
            .apiKey(System.getenv("OVH_AI_ENDPOINTS_ACCESS_TOKEN"))
            .modelName(System.getenv("OVH_AI_ENDPOINTS_MODEL_NAME"))
            .baseUrl(System.getenv("OVH_AI_ENDPOINTS_MODEL_URL"))
            .maxTokens(512)
            .temperature(0.0)
            .logRequests(false)
            .logResponses(false)
            .build();

snippet java-12:
  name: "Memory memory storage"
  prefix: "java-12"
  scope: "java"
  body: |
    ChatMemory chatMemory = MessageWindowChatMemory.withMaxMessages(10);

snippet java-13:
  name: "Memory assistant"
  prefix: "java-13"
  scope: "java"
  body: |
    Assistant assistant = AiServices.builder(Assistant.class)
        .streamingChatModel(steamingModel)
        .chatMemory(chatMemory)
        .build();  

snippet java-14:
  name: "Memory call"
  prefix: "java-14"
  scope: "java"
  body: |
    _LOG.info("üí¨: My name is St√©phane.\n");
    TokenStream tokenStream = assistant.chat("My name is St√©phane.");
    CompletableFuture<ChatResponse> futureChatResponse = new CompletableFuture<>();
    _LOG.info("ü§ñ: ");
    tokenStream
            .onPartialResponse(_LOG::info)
            .onCompleteResponse(token -> {
                _LOG.info("\nüí¨: Do you remember what is my name?\n");
                _LOG.info("ü§ñ: ");
                assistant.chat("Do you remember what is my name?")
                        .onPartialResponse(_LOG::info)
                        .onCompleteResponse(response -> futureChatResponse.complete(response))
                        .onError(Throwable::printStackTrace).start();
            })
            .onError(Throwable::printStackTrace).start();
    futureChatResponse.join();

snippet java-15:
  name: "LangChain4j OVHai dependency"
  prefix: "java-15"
  scope: "xml"
  body: |
    <dependency>
      <groupId>dev.langchain4j</groupId>
      <artifactId>langchain4j-ovh-ai</artifactId>
      <version>${langchain4j.beta.version}</version>
    </dependency>

snippet java-16:
  name: "RAG AI Services"
  prefix: "java-16"
  scope: "java"
  body: |
    interface Assistant {
      @SystemMessage("You are Nestor, a virtual assistant. Answer to the question.")
      TokenStream chat(String message);
    }

snippet java-17:
  name: "RAG model"
  prefix: "java-17"
  scope: "java"
  body: |
    StreamingChatModel steamingModel = OpenAiStreamingChatModel.builder()
        .apiKey(System.getenv("OVH_AI_ENDPOINTS_ACCESS_TOKEN"))
        .modelName(System.getenv("OVH_AI_ENDPOINTS_MODEL_NAME"))
        .baseUrl(System.getenv("OVH_AI_ENDPOINTS_MODEL_URL"))
        .maxTokens(512)
        .temperature(0.0)
        .logRequests(false)
        .logResponses(false)
        .build();

snippet java-18:
  name: "RAG memory"
  prefix: "java-18"
  scope: "java"
  body: |
    ChatMemory chatMemory = MessageWindowChatMemory.withMaxMessages(10);

snippet java-19:
  name: "RAG chunk"
  prefix: "java-19"
  scope: "java"
  body: |
    DocumentParser documentParser = new TextDocumentParser();
    Document document = loadDocument("./resources/rag-files/conference-information-talk-01.md",
            documentParser);
    DocumentSplitter splitter = DocumentSplitters.recursive(8000, 50);

    List<TextSegment> segments = splitter.split(document);

snippet java-20:
  name: "RAG embedding"
  prefix: "java-20"
  scope: "java"
  body: |
    EmbeddingModel embeddingModel = OvhAiEmbeddingModel.builder()
                    .apiKey(System.getenv("OVH_AI_ENDPOINTS_ACCESS_TOKEN"))
                    .baseUrl(System.getenv("OVH_AI_ENDPOINTS_EMBEDDING_MODEL"))
                    .build();
    List<Embedding> embeddings = embeddingModel.embedAll(segments).content();

snippet java-21:
  name: "RAG embedding store"
  prefix: "java-21"
  scope: "java"
  body: |
    EmbeddingStore<TextSegment> embeddingStore = new InMemoryEmbeddingStore<>();
    embeddingStore.addAll(embeddings, segments);
    ContentRetriever contentRetriever = EmbeddingStoreContentRetriever.builder()
        .embeddingStore(embeddingStore)
        .embeddingModel(embeddingModel)
        .maxResults(3)
        .minScore(0.1)
        .build();

snippet java-22:
  name: "RAG assistant"
  prefix: "java-22"
  scope: "java"
  body: |
    Assistant assistant = AiServices.builder(Assistant.class)
        .streamingChatModel(steamingModel)
        .chatMemory(chatMemory)
        .contentRetriever(contentRetriever)
        .build();

snippet java-23:
  name: "RAG call"
  prefix: "java-23"
  scope: "java"
  body: |
    _LOG.info("üí¨: What is the program at AI Summit Barcelona?\n");
    TokenStream tokenStream = assistant.chat("What is the program at AI Summit Barcelona?");
    CompletableFuture<ChatResponse> futureChatResponse = new CompletableFuture<>();

    _LOG.info("ü§ñ: ");
    tokenStream
        .onCompleteResponse(response -> futureChatResponse.complete(response))
        .onPartialResponse(_LOG::info)
        .onError(Throwable::printStackTrace).start();
    futureChatResponse.join();

snippet java-24:
  name: "Tool generateImage tool"
  prefix: "java-24"
  scope: "java"
  body: |
    // Define the tool using the @Tool annotation
    @Tool("""
                Tool to create an image with Stable Diffusion XL given a prompt and a negative prompt.
                """)
    void generateImage(@P("Prompt that explains the image") String prompt, @P("Negative prompt that explains what the image must not contains") String negativePrompt) throws IOException, InterruptedException {
        _LOG.info("Prompt: {}", prompt);
        _LOG.info("Negative prompt: {}", negativePrompt);

        // java-25

        // java-26
    }

snippet java-25:
  name: "Tool call SDXL"
  prefix: "java-25"
  scope: "java"
  body: |
    // Call Stable diffusion model with the prompt and negative prompt
    HttpRequest httpRequest = HttpRequest.newBuilder()
            .uri(URI.create(System.getenv("OVH_AI_ENDPOINTS_SD_URL")))
            .POST(HttpRequest.BodyPublishers.ofString("""
                        {"prompt": "%s", 
                          "negative_prompt": "%s"}
                        """.formatted(prompt, negativePrompt)))
            .header("accept", "application/octet-stream")
            .header("Content-Type", "application/json")
            .header("Authorization", "Bearer " + System.getenv("OVH_AI_ENDPOINTS_ACCESS_TOKEN"))
            .build();

snippet java-26:
  name: "Tool: save image"
  prefix: "java-26"
  scope: "java"
  body: |
    // Create the image file from Stable Diffusion response
    HttpResponse<byte[]> response = HttpClient.newHttpClient()
            .send(httpRequest, HttpResponse.BodyHandlers.ofByteArray());

    _LOG.debug("SDXL status code: {}", response.statusCode());
    Files.write(Path.of("generated-image.jpeg"), response.body());
    _LOG.info(String.format("\nüñºÔ∏è Image generated: %s", Path.of("generated-image.jpeg")));

snippet java-27:
  name: "Tool: AI Services"
  prefix: "java-27"
  scope: "java"
  body: |
    @SystemMessage("""
            Your are an expert of using the Stable Diffusion XL model.
            You can use the generateImage function that takes as parameter the prompt 
            (you can optimize it) and the negative prompt (you need to create it from the optimized user prompt).
            """)
    @UserMessage("Create an image with stable diffusion XL following this description: {{userMessage}}")
    String chat(@V("userMessage") String userMessage);

snippet java-28:
  name: "Tool: model selection"
  prefix: "java-28"
  scope: "java"
  body: |
    ChatModel chatModel = OpenAiChatModel.builder()
            .apiKey(System.getenv("OVH_AI_ENDPOINTS_ACCESS_TOKEN"))
            .baseUrl(System.getenv("OVH_AI_ENDPOINTS_MODEL_URL"))
            .modelName(System.getenv("OVH_AI_ENDPOINTS_MODEL_NAME"))
            .logRequests(false)
            .logResponses(false)
            // To have more deterministic outputs, set temperature to 0.
            .temperature(0.0)
            // Stable Diffusion could take a while to generate images
            .timeout(Duration.ofMinutes(5))
            .build();


snippet java-29:
  name: "Tool: memory"
  prefix: "java-29"
  scope: "java"
  body: |
    // Add memory to fine tune the SDXL prompt.
    ChatMemory chatMemory = MessageWindowChatMemory.withMaxMessages(10);

snippet java-30:
  name: "Tool: build chatbot"
  prefix: "java-30"
  scope: "java"
  body: |
    // Build the chatbot thanks to LangChain4J AI Services mode (see https://docs.langchain4j.dev/tutorials/ai-services)
    ChatBot chatBot = AiServices.builder(ChatBot.class)
            .chatModel(chatModel)
            .tools(new ImageGenTools())
            .chatMemory(chatMemory)
            .build();

snippet java-31:
  name: "Tool: call LLM"
  prefix: "java-31"
  scope: "java"
  body: |
    // Start the conversation loop (enter "exit" to quit)
    String userInput = "";
    Scanner scanner = new Scanner(System.in);
    while (true) {
        _LOG.info("\nEnter your message: ");
        userInput = scanner.nextLine();
        if (userInput.equalsIgnoreCase("exit")) break;
        _LOG.info("\nResponse: " + chatBot.chat(userInput));
    }
    scanner.close();

snippet java-32:
  name: "MCP: dependency"
  prefix: "java-32"
  scope: "xml"
  body: |
    <dependency>
      <groupId>dev.langchain4j</groupId>
      <artifactId>langchain4j-mcp</artifactId>
      <version>1.4.0-beta10</version>
    </dependency>

snippet java-33:
  name: "MCP: AI Services"
  prefix: "java-33"
  scope: "java"
  body: |
    @SystemMessage("""
            Your are an expert of using the Stable Diffusion XL model.
            You can use the generateImage function that takes as parameter the prompt 
            (you can optimize it) and the negative prompt (you need to create it from the optimized user prompt).
            """)
    @UserMessage("Create an image with stable diffusion XL following this description: {{userMessage}}")
    String chat(@V("userMessage") String userMessage);

snippet java-34:
  name: "MCP: model selection"
  prefix: "java-34"
  scope: "java"
  body: |
    ChatModel chatModel = OpenAiChatModel.builder()
        .apiKey(System.getenv("OVH_AI_ENDPOINTS_ACCESS_TOKEN"))
        .baseUrl(System.getenv("OVH_AI_ENDPOINTS_MODEL_URL"))
        .modelName(System.getenv("OVH_AI_ENDPOINTS_MODEL_NAME"))
        .logRequests(false)
        .logResponses(false)
        // To have more deterministic outputs, set temperature to 0.
        .temperature(0.0)
        // Stable Diffusion could take a while to generate images
        .timeout(Duration.ofMinutes(5))
        .build();


snippet java-35:
  name: "MCP: transport"
  prefix: "java-35-mcp-transport"
  scope: "java"
  body: |
    McpTransport transport = new StreamableHttpMcpTransport.Builder()
            // https://xxxx/mcp/sse
            .url(System.getenv("MCP_SERVER_URL"))
            .logRequests(true)
            .logResponses(true)
            .build();

snippet java-36:
  name: "MCP: mcp client"
  prefix: "java-36-mcp-client"
  scope: "java"
  body: |
    McpClient mcpClient = new DefaultMcpClient.Builder()
            .transport(transport)
            .build();

snippet java-37:
  name: "MCP: tool provider"
  prefix: "java-37-mcp-tool"
  scope: "java"
  body: |
    McpToolProvider toolProvider = McpToolProvider.builder()
            .mcpClients(mcpClient)
            .build();

snippet java-38:
  name: "MCP: memory"
  prefix: "java-38-memory"
  scope: "java"
  body: |
    ChatMemory chatMemory = MessageWindowChatMemory.withMaxMessages(10);

snippet java-39:
  name: "MCP: chatbot"
  prefix: "java-39-chatbot"
  scope: "java"
  body: |
    ChatBot bot = AiServices.builder(ChatBot.class)
            .chatModel(chatModel)
            .toolProvider(toolProvider)
            .chatMemory(chatMemory)
            .build();

snippet java-40:
  name: "MCP: call LLM"
  prefix: "java-40-prompt"
  scope: "java"
  body: |
    String userInput = "";
    Scanner scanner = new Scanner(System.in);
    while (true) {
        _LOG.info("\nEnter your message: ");
        userInput = scanner.nextLine();
        if (userInput.equalsIgnoreCase("exit")) break;
        _LOG.info("\nResponse: " + bot.chat(userInput));
    }
    scanner.close();

## java-quarkus section
snippet quarkus-01:
  name: "Quarkus properties"
  prefix: "quarkus-01"
  body: |
    quarkus.langchain4j.mistralai.base-url=\${OVH_AI_ENDPOINTS_MODEL_URL}
    quarkus.langchain4j.mistralai.api-key=\${OVH_AI_ENDPOINTS_ACCESS_TOKEN}
    quarkus.langchain4j.mistralai.chat-model.max-tokens=512
    quarkus.langchain4j.mistralai.chat-model.model-name=\${OVH_AI_ENDPOINTS_MODEL_NAME}
    quarkus.langchain4j.mistralai.log-requests=true
    quarkus.langchain4j.mistralai.log-responses=false
    quarkus.langchain4j.mistralai.chat-model.temperature=0.2

snippet quarkus-02:
  name: "Class annotation simple"
  prefix: "quarkus-02"
  scope: "java"
  body: |
    @RegisterAiService

snippet quarkus-03:
  name: "Entry point to llm simple"
  prefix: "quarkus-03"
  scope: "java"
  body: |
    @SystemMessage("You are a virtual assistant and your name is Nestor.")
    @UserMessage("Answer as best possible to the following question: {question}. The answer must be in a style of a virtual assistant.")
    String askAQuestion(String question);

snippet quarkus-04:
  name: "Class annotation simple resource"
  prefix: "quarkus-04"
  scope: "java"
  body: |
    @Path("/chatbot")

snippet quarkus-05:
  name: "Injection simple resource"
  prefix: "quarkus-05"
  scope: "java"
  body: |
    @Inject
    AISimpleService aiEndpointService;

snippet quarkus-06:
  name: "Simple resource ask method"
  prefix: "quarkus-06"
  scope: "java"
  body: |
    @Path("simple")
    @POST
    public String ask(String question) {
      // Call the askAQuestion method of the AISimpleService service
      return aiEndpointService.askAQuestion(question);
    }

snippet quarkus-07:
  name: "Class annotation advanced"
  prefix: "quarkus-07"
  scope: "java"
  body: |
    @RegisterAiService

snippet quarkus-08:
  name: "Entry point to llm advanced"
  prefix: "quarkus-08"
  scope: "java"
  body: |
    @SystemMessage("You are a virtual assistant and your name is Nestor.")
    @UserMessage("Answer as best possible to the following question: {question}. The answer must be in a style of a virtual assistant.")
    Multi<String> askAQuestion(String question);

snippet quarkus-09:
  name: "Class annotation advanced resource"
  prefix: "quarkus-09"
  scope: "java"
  body: |
    @Path("/chatbot")

snippet quarkus-10:
  name: "Injection advanced resource"
  prefix: "quarkus-10"
  scope: "java"
  body: |
    @Inject
    AIAdvancedService advancedService;

snippet quarkus-11:
  name: "Advanced resource ask method"
  prefix: "quarkus-11"
  scope: "java"
  body: |
    @Path("advanced")
    @POST
    public Multi<String> ask(String question) {
      // Call the askAQuestion method of the AISimpleService service and stream the
      // answer, see https://quarkus.io/guides/getting-started-reactive
      return advancedService.askAQuestion(question);
    }

snippet quarkus-12:
  name: "Class annotation memory"
  prefix: "quarkus-12"
  scope: "java"
  body: |
    @ApplicationScoped
    @RegisterAiService

snippet quarkus-13:
  name: "Entry point to llm memory"
  prefix: "quarkus-13"
  scope: "java"
  body: |
    @SystemMessage("You are a virtual assistant and your name is Nestor.")
    @UserMessage("Answer as best possible to the following question: {question}. The answer must be in a style of a virtual assistant.")
    Multi<String> askAQuestion(String question, @MemoryId String memoryId);

snippet quarkus-14:
  name: "Class annotation memory resource"
  prefix: "quarkus-14"
  scope: "java"
  body: |
    @Path("/chatbot")

snippet quarkus-15:
  name: "Injection memory resource"
  prefix: "quarkus-15"
  scope: "java"
  body: |
    @Inject
    AIMemoryService aiMemoryService;

snippet quarkus-16:
  name: "Memory resource ask method"
  prefix: "quarkus-16"
  scope: "java"
  body: |
    @Path("memory")
    @POST
    public Multi<String> ask(String question) {
      // Call the askAQuestion method of the AISimpleService service and stream the
      // answer, see https://quarkus.io/guides/getting-started-reactive
      return aiMemoryService.askAQuestion(question, "user-one");
    }

snippet quarkus-17:
  name: "quarkus-17-dep-mcp"
  prefix: "quarkus-17"
  scope: "xml"
  body: |
    <dependency>
        <groupId>io.quarkiverse.mcp</groupId>
        <artifactId>quarkus-mcp-server-sse</artifactId>
        <version>1.5.3</version>
    </dependency>

snippet quarkus-18:
  name: "quarkus-18-prop-mcp"
  prefix: "quarkus-18"
  body: |
    # MCP configuration
    quarkus.rest-client."com.ovhcloud.ai.quarkus.chatbot.service.StableDiffusionService".url=\${OVH_AI_ENDPOINTS_SD_URL}

snippet quarkus-19:
  name: "quarkus-19-svc-sdxl"
  prefix: "quarkus-19"
  scope: "java"
  body: |
    @RegisterRestClient
    @ClientHeaderParam(name = "Content-Type", value = "application/json")

snippet quarkus-20:
  name: "quarkus-20-call-sdxl"
  prefix: "quarkus-20"
  scope: "java"
  body: |
    @POST
    @ClientHeaderParam(name = "Authorization", value = "Bearer ${quarkus.langchain4j.mistralai.api-key}")
    byte[] generateImage(SDPayload payload);

snippet quarkus-21:
  name: "quarkus-21-inject-sdxl"
  prefix: "quarkus-21"
  scope: "java"
  body: |
    @RestClient
    StableDiffusionService stableDiffusionService;

snippet quarkus-22:
  name: "quarkus-22-tool"
  prefix: "quarkus-22"
  scope: "java"
  body: |
    // Define the tool using the @Tool annotation
    @Tool(description = "Tool to create an image with Stable Diffusion XL given a prompt and a negative prompt.")
    String generateImage(@P("Prompt that explains the image") String prompt, @P("Negative prompt that explains what the image must not contains") String negativePrompt) throws IOException, InterruptedException {
        _LOG.info("Prompt: {}", prompt);
        _LOG.info("Negative prompt: {}", negativePrompt);

        // quarkus-23
        return "Image generated";
    }

snippet quarkus-23:
  name: "quarkus-23-gen-img"
  prefix: "quarkus-23"
  scope: "java"
  body: |
    byte[] image = stableDiffusionService.generateImage(new SDPayload(prompt, negativePrompt));

    Files.write(Path.of("generated-image.jpeg"), image);

##### Python section
snippet py-33:
  name: "Chatbot requirements"
  prefix: "py-33"
  body: | 
    # LangChain dependencies
    langchain==0.3.27
    langgraph==0.3.27
    langchain-mistralai==0.2.11
    langchain_chroma==0.2.5
    langchain-community==0.3.27
    langchain-mcp-adapters==0.1.9

snippet py-34:
  name: "Chatbot simple model"
  prefix: "py-34"
  scope: "python"
  body: | 
    model = ChatMistralAI(
        base_url=os.getenv("OVH_AI_ENDPOINTS_MODEL_URL"),
        api_key=os.getenv("OVH_AI_ENDPOINTS_ACCESS_TOKEN"),
        model=os.getenv("OVH_AI_ENDPOINTS_MODEL_NAME"),
        max_tokens=512,
        temperature=0
    )

snippet py-35:
  name: "Chatbot simple messages"
  prefix: "py-35"
  scope: "python"
  body: | 
    messages = [
        (
            "system",
            "You are Nestor, a virtual assistant. Answer to the question.",
        ),
        ("human", "Tell me a joke about Python developers,"),
    ]

snippet py-36:
  name: "Chatbot simple model call"
  prefix: "py-36"
  scope: "python"
  body: | 
    ai_msg = model.invoke(messages)

snippet py-37:
  name: "Chatbot simple print response"
  prefix: "py-37"
  scope: "python"
  body: | 
    print("üí¨: Question: Tell me a joke about Python developers\n")
    print(f"ü§ñ: {ai_msg.content}")

snippet py-38:
  name: "Chatbot stream model"
  prefix: "py-38"
  scope: "python"
  body: | 
    model = ChatMistralAI(
        base_url=os.getenv("OVH_AI_ENDPOINTS_MODEL_URL"),
        api_key=os.getenv("OVH_AI_ENDPOINTS_ACCESS_TOKEN"),
        model=os.getenv("OVH_AI_ENDPOINTS_MODEL_NAME"),
        max_tokens=512,
        temperature=0
    )

snippet py-39:
  name: "Chatbot stream messages"
  prefix: "py-39"
  scope: "python"
  body: | 
    messages = [
        (
            "system",
            "You are Nestor, a virtual assistant. Answer to the question.",
        ),
        ("human", "Tell me a joke about Python developers"),
    ]

snippet py-40:
  name: "Chatbot stream print"
  prefix: "py-40"
  scope: "python"
  body: | 
    print("üí¨: Question: Tell me a joke about Python developers\n")
    print("ü§ñ:")

snippet py-41:
  name: "Chatbot stream call model"
  prefix: "py-41"
  scope: "python"
  body: | 
    for chunk in model.stream(messages):
      print(chunk.content, end="", flush=True)

snippet py-42:
  name: "Chatbot memory model"
  prefix: "py-42"
  scope: "python"
  body: | 
    model = ChatMistralAI(
        base_url=os.getenv("OVH_AI_ENDPOINTS_MODEL_URL"),
        api_key=os.getenv("OVH_AI_ENDPOINTS_ACCESS_TOKEN"),
        model=os.getenv("OVH_AI_ENDPOINTS_MODEL_NAME"),
        max_tokens=512,
        temperature=0
    )

snippet py-43:
  name: "Chatbot memory LangGraph creation"
  prefix: "py-43"
  scope: "python"
  body: | 
    workflow = StateGraph(state_schema=MessagesState)

snippet py-44:
  name: "Chatbot memory function call"
  prefix: "py-44"
  scope: "python"
  body: | 
    def call_model(state: MessagesState):
      response = model.invoke(
          [SystemMessage("You are Nestor, a virtual assistant. Answer to the question.")] 
          + state["messages"]
      )

      return {"messages": response.content}

snippet py-45:
  name: "Chatbot memory nodes"
  prefix: "py-45"
  scope: "python"
  body: | 
    workflow.add_edge(START, "model")
    workflow.add_node("model", call_model)

snippet py-46:
  name: "Chatbot memory add memory"
  prefix: "py-46"
  scope: "python"
  body: | 
    memory = MemorySaver()

snippet py-47:
  name: "Chatbot memory graph compile"
  prefix: "py-47"
  scope: "python"
  body: | 
    app = workflow.compile(
      checkpointer=memory
    )

snippet py-48:
  name: "Chatbot memory thread ID"
  prefix: "py-48"
  scope: "python"
  body: | 
    thread_id = uuid.uuid4()
    config = {"configurable": {"thread_id": thread_id}}

snippet py-49:
  name: "Chatbot memory model call"
  prefix: "py-49"
  scope: "python"
  body: | 
    question = "Hello, my name is St√©phane"
    print(f"üë§: {question}")
    print("ü§ñ: ")
    for message_chunk, metadata in app.stream( 
        {"messages": "Hello, my name is St√©phane"}, config,
        stream_mode="messages",
    ):
        if message_chunk.content:
            print(message_chunk.content, end="", flush=True)
    print()

    question = "Do you remember my name?"
    print(f"üë§: {question}")
    print("ü§ñ: ")
    for message_chunk, metadata in app.stream( 
        {"messages": "Do you remember my name?"}, config,
        stream_mode="messages",
    ):
        if message_chunk.content:
            print(message_chunk.content, end="", flush=True)
    print()

snippet py-50:
  name: "Chatbot RAG model"
  prefix: "py-50"
  scope: "python"
  body: | 
    model = ChatMistralAI(
    base_url=os.getenv("OVH_AI_ENDPOINTS_MODEL_URL"),
    api_key=os.getenv("OVH_AI_ENDPOINTS_ACCESS_TOKEN"),
    model=os.getenv("OVH_AI_ENDPOINTS_MODEL_NAME"),
    max_tokens=512,
    temperature=0
    )

snippet py-51:
  name: "Chatbot RAG doc loading"
  prefix: "py-51"
  scope: "python"
  body: | 
    loader = TextLoader("./rag-files/conference-information-talk-01.md")
    docs = loader.load()

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    splits = text_splitter.split_documents(docs)

snippet py-52:
  name: "Chatbot RAG vector DB"
  prefix: "py-52"
  scope: "python"
  body: | 
    vectorstore = Chroma.from_documents(documents=splits, 
                   embedding=OVHCloudEmbeddings(model_name="bge-multilingual-gemma2", 
                   access_token=os.getenv("OVH_AI_ENDPOINTS_ACCESS_TOKEN")))

snippet py-53:
  name: "Chatbot RAG prompt"
  prefix: "py-53"
  scope: "python"
  body: | 
    prompt = hub.pull("rlm/rag-prompt")

snippet py-54:
  name: "Chatbot RAG chain"
  prefix: "py-54"
  scope: "python"
  body: | 
    rag_chain = (
      {"context": vectorstore.as_retriever(), "question": RunnablePassthrough()} |
      prompt
      | model
    )

snippet py-55:
  name: "Chatbot RAG print"
  prefix: "py-55"
  scope: "python"
  body: | 
    print("üë§: What is the program at AI Summit Barcelona?")
    print("ü§ñ:")
    for r in rag_chain.stream({"question", "What is the program at AI Summit Barcelona?"}):
      print(r.content, end="", flush=True)
      time.sleep(0.150)

snippet py-56:
  name: "Image gen model"
  prefix: "py-56"
  scope: "python"
  body: | 
    model = ChatMistralAI(
      base_url=os.getenv("OVH_AI_ENDPOINTS_MODEL_URL"),
      api_key=os.getenv("OVH_AI_ENDPOINTS_ACCESS_TOKEN"),
      model=os.getenv("OVH_AI_ENDPOINTS_MODEL_NAME"),
      temperature=0
    )

snippet py-57:
  name: "Image gen tool"
  prefix: "py-57"
  scope: "python"
  body: | 
    @tool
    def generateImage(prompt: str, negative_prompt: str) -> str:
        # Give a detailed description of the tool to "help" the model
        # py-58

        print("‚ö°Ô∏è Generate image with Stable Diffusion XL üèûÔ∏è")

        # Payload and headers to send to SDXL API
        # py-59        

        # SDXL call and image save on the file system
        file_path = "./generated-image.png"
        
        # py-60

        return f"üñºÔ∏è Image generated: {file_path}"

snippet py-58:
  name: "Image gen tool description"
  prefix: "py-58"
  scope: "python"
  body: | 
    """Tool to create an image with Stable Diffusion XL given a prompt and a negative prompt.
      - prompt: Prompt that explains the image
      - negative_prompt: Negative prompt that explains what the image must not contains
    """

snippet py-59:
  name: "Image gen tool payload"
  prefix: "py-59"
  scope: "python"
  body: | 
    data = {
        "prompt": prompt,
        "negative_prompt": negative_prompt
    }

    headers = {
        "accept": "application/octet-stream",
        "content-type": "application/json",
        "Authorization": f"Bearer {os.getenv('OVH_AI_ENDPOINTS_ACCESS_TOKEN')}",
    }

snippet py-60:
  name: "Image gen tool request"
  prefix: "py-60"
  scope: "python"
  body: | 
    response = requests.post(os.getenv("OVH_AI_ENDPOINTS_SD_URL"), headers=headers, data=json.dumps(data))
    if response.status_code == 200:
        # Handle response
        response_data = response.content
        with open(file_path, 'wb') as file:
          file.write(response_data)
    else:
        print("Error:", response.status_code)
        return f"‚ùå Unable to generate image ({file_path}): {response.status_code} ‚ùå"

snippet py-61:
  name: "Image gen model tools"
  prefix: "py-61"
  scope: "python"
  body: | 
    tools = [generateImage]

    model_with_tools = model.bind_tools(tools)

snippet py-62:
  name: "Image gen messages"
  prefix: "py-62"
  scope: "python"
  body: | 
    question = "A red cat on a couch"
    messages = [HumanMessage(role="user", content=question), 
            SystemMessage(role="system", content="""
        Your are an expert of using the Stable Diffusion XL model.
        The user explains in natural language what kind of image he wants.
        You must do the following steps:
          - Understand the user's request.
          - Generate the two kinds of prompts for stable diffusion: the prompt and the negative prompt
          - the prompts must be in english and detailed and optimized for the Stable Diffusion XL model. 
          - once and only once you have this two prompts call the tool with the two prompts.
        If asked about to create an image, you MUST call the `generateImage` function.
        """)]

snippet py-63:
  name: "Image gen first model call"
  prefix: "py-63"
  scope: "python"
  body: | 
    print(f"üí¨: {question}")

    ai_msg = model_with_tools.invoke(messages)
    print(f"ü§ñ: {ai_msg.content}")

    messages.append({
           "role": "assistant",
           "type": "message",
           "content": ai_msg.content
       })

snippet py-64:
  name: "Image gen tool call"
  prefix: "py-64"
  scope: "python"
  body: | 
    for tool_call in ai_msg.tool_calls:
        selected_tool = {"generateImage": generateImage}[tool_call["name"]]
        tool_result = selected_tool.invoke(tool_call)
        messages.append(tool_result)

snippet py-65:
  name: "Image gen final model call"
  prefix: "py-65"
  scope: "python"
  body: | 
    ai_msg = model_with_tools.invoke(messages)

snippet py-66:
  name: "Image gen print final response"
  prefix: "py-66"
  scope: "python"
  body: | 
    print(f"ü§ñ: {ai_msg.content}")

snippet py-67:
  name: "MCP server tool"
  prefix: "py-67"
  scope: "python"
  body: | 
    # Give a detailed description of the tool to "help" the model
    """Tool to create an image with Stable Diffusion XL given a prompt and a negative prompt.
      - prompt: Prompt that explains the image
      - negative_prompt: Negative prompt that explains what the image must not contains
    """
    print("‚ö°Ô∏è Generate image with Stable Diffusion XL üèûÔ∏è")

    # Payload and headers to send to SDXL API
    data = {
        "prompt": prompt,
        "negative_prompt": negative_prompt
    }

    headers = {
        "accept": "application/octet-stream",
        "content-type": "application/json",
        "Authorization": f"Bearer {os.getenv('OVH_AI_ENDPOINTS_ACCESS_TOKEN')}",
    }

    # SDXL call and image save on the file system
    file_path = "./generated-image-mcp.png"

    response = requests.post(os.getenv("OVH_AI_ENDPOINTS_SD_URL"), headers=headers, data=json.dumps(data))
    if response.status_code == 200:
        # Handle response
        response_data = response.content
        with open(file_path, 'wb') as file:
          file.write(response_data)
    else:
        print("Error:", response.status_code)
        return f"‚ùå Unable to generate image ({file_path}): {response.status_code} ‚ùå"

    return f"üñºÔ∏è Image generated: {file_path}"

snippet py-68:
  name: "MCP client client"
  prefix: "py-68"
  scope: "python"
  body: | 
    client = MultiServerMCPClient(
        {
            "gen_images": {
                "transport": "streamable_http",
                "url": "http://localhost:8000/mcp/"
            },
        }
    )

snippet py-69:
  name: "MCP client model"
  prefix: "py-69"
  scope: "python"
  body: | 
    model = ChatMistralAI(
        base_url=os.getenv("OVH_AI_ENDPOINTS_MODEL_URL"),
        api_key=os.getenv("OVH_AI_ENDPOINTS_ACCESS_TOKEN"),
        model=os.getenv("OVH_AI_ENDPOINTS_MODEL_NAME"),
        temperature=0
    )

snippet py-70:
  name: "MCP client configure tools"
  prefix: "py-70"
  scope: "python"
  body: |
    tools = await client.get_tools()
    model_with_tools = model.bind_tools(tools)

snippet py-71:
  name: "MCP client messages"
  prefix: "py-71"
  scope: "python"
  body: |
    question = "A red cat with green eyes on a couch"
    messages = [HumanMessage(role="user", content=question), 
                SystemMessage(role="system", content="""
            Your are an expert of using the Stable Diffusion XL model.
            The user explains in natural language what kind of image he wants.
            You must do the following steps:
              - Understand the user's request.
              - Generate the two kinds of prompts for stable diffusion: the prompt and the negative prompt
              - the prompts must be in english and detailed and optimized for the Stable Diffusion XL model. 
              - once and only once you have this two prompts call the tool with the two prompts.
            If asked about to create an image, you MUST call the `generateImage` function.
            """)]

snippet py-72:
  name: "MCP client first model call"
  prefix: "py-72"
  scope: "python"
  body: |
    print(f"üí¨: {question}")

    ai_msg = await model_with_tools.ainvoke(messages)
    messages.append({
          "role": "assistant",
          "type": "message",
          "content": ai_msg.content
      })

snippet py-73:
  name: "MCP client tool calling"
  prefix: "py-73"
  scope: "python"
  body: |
    for tool_call in ai_msg.tool_calls:
        selected_tool = {"generateImage": tools}[tool_call["name"]]
        tool_result = await selected_tool[0].ainvoke(tool_call)
        messages.append(tool_result)

snippet py-74:
  name: "MCP client last model call"
  prefix: "py-74"
  scope: "python"
  body: |
    ai_msg = model_with_tools.invoke(messages)

snippet py-75:
  name: "MCP client print response"
  prefix: "py-75"
  scope: "python"
  body: |
    print(f"ü§ñ:{ai_msg.content}")