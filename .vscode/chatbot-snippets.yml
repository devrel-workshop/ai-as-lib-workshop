# java-langchain4j section

snippet java-01:
  name: "java LangChain4J dependencies"
  prefix: "java-01"
  scope: "xml"
  body: |
    <dependency>
      <groupId>dev.langchain4j</groupId>
      <artifactId>langchain4j</artifactId>
      <version>${langchain4j.version}</version>
    </dependency>

    <dependency>
      <groupId>dev.langchain4j</groupId>
      <artifactId>langchain4j-mistral-ai</artifactId>
      <version>${langchain4j.provider.version}</version>
    </dependency>

snippet java-02:
  name: "Simple chatbot AI Services"
  prefix: "java-02"
  scope: "java"
  body: |
    interface Assistant {
      @SystemMessage("You are Nestor, a virtual assistant. Answer to the question.")
      String chat(String message);
    }

snippet java-03:
  name: "Simple chatbot model"
  prefix: "java-03"
  scope: "java"
  body: |
    MistralAiChatModel chatModel = MistralAiChatModel.builder()
        .apiKey(System.getenv("OVH_AI_ENDPOINTS_ACCESS_TOKEN"))
        .modelName(System.getenv("OVH_AI_ENDPOINTS_MODEL_NAME"))
        .baseUrl(System.getenv("OVH_AI_ENDPOINTS_MODEL_URL"))
        .maxTokens(512)
        .temperature(0.0)
        .logRequests(true)
        .logResponses(true)
        .build();

snippet java-04:
  name: "Simple chatbot assistant"
  prefix: "java-04"
  scope: "java"
  body: |
    Assistant assistant = AiServices.builder(Assistant.class)
        .chatModel(chatModel)
        .build();

snippet java-05:
  name: "Simple chatbot call"
  prefix: "java-05"
  scope: "java"
  body: |
    _LOG.info("💬: Question: Tell me a joke about Java developers\n");
    _LOG.info("🤖: {}", assistant.chat("Tell me a joke about Java developers"));

snippet java-06:
  name: "Streaming AI Services"
  prefix: "java-06"
  scope: "java"
  body: |
    interface Assistant {
      @SystemMessage("You are Nestor, a virtual assistant. Answer to the question.")
      TokenStream chat(String message);
    }

snippet java-07:
  name: "Streaming model"
  prefix: "java-07"
  scope: "java"
  body: |
    MistralAiStreamingChatModel steamingModel = MistralAiStreamingChatModel.builder()
        .apiKey(System.getenv("OVH_AI_ENDPOINTS_ACCESS_TOKEN"))
        .modelName(System.getenv("OVH_AI_ENDPOINTS_MODEL_NAME"))
        .baseUrl(System.getenv("OVH_AI_ENDPOINTS_MODEL_URL"))
        .maxTokens(512)
        .temperature(0.0)
        .logRequests(false)
        .logResponses(false)
        .build();

snippet streaming-assistant:
  name: "Streaming assistant"
  prefix: "java-08"
  scope: "java"
  body: |
    Assistant assistant = AiServices.builder(Assistant.class)
        .streamingChatModel(steamingModel)
        .build();

snippet java-09:
  name: "Streaming call"
  prefix: "java-09"
  scope: "java"
  body: |
    _LOG.info("💬: Tell me a joke about Java developers\n");
    TokenStream tokenStream = assistant.chat("Tell me a joke about Java developers");
    CompletableFuture<ChatResponse> futureChatResponse = new CompletableFuture<>();

    _LOG.info("🤖: ");
    tokenStream
        .onCompleteResponse(response -> futureChatResponse.complete(response))
        .onPartialResponse(_LOG::info)
        .onError(Throwable::printStackTrace).start();
    futureChatResponse.join();

snippet java-10:
  name: "Memory AI Services"
  prefix: "java-10"
  scope: "java"
  body: |
    interface Assistant {
      @SystemMessage("You are Nestor, a virtual assistant. Answer to the question.")
      TokenStream chat(String message);
    }

snippet java-11:
  name: "Memory model"
  prefix: "java-11"
  scope: "java"
  body: |
    MistralAiStreamingChatModel steamingModel = MistralAiStreamingChatModel.builder()
        .apiKey(System.getenv("OVH_AI_ENDPOINTS_ACCESS_TOKEN"))
        .modelName(System.getenv("OVH_AI_ENDPOINTS_MODEL_NAME"))
        .baseUrl(System.getenv("OVH_AI_ENDPOINTS_MODEL_URL"))
        .maxTokens(512)
        .temperature(0.0)
        .logRequests(false)
        .logResponses(false)
        .build();

snippet java-12:
  name: "Memory memory storage"
  prefix: "java-12"
  scope: "java"
  body: |
    ChatMemory chatMemory = MessageWindowChatMemory.withMaxMessages(10);

snippet java-13:
  name: "Memory assistant"
  prefix: "java-13"
  scope: "java"
  body: |
    Assistant assistant = AiServices.builder(Assistant.class)
        .streamingChatModel(steamingModel)
        .chatMemory(chatMemory)
        .build();  

snippet java-14:
  name: "Memory call"
  prefix: "java-14"
  scope: "java"
  body: |
    _LOG.info("💬: My name is Stéphane.\n");
    TokenStream tokenStream = assistant.chat("My name is Stéphane.");
    CompletableFuture<ChatResponse> futureChatResponse = new CompletableFuture<>();
    _LOG.info("🤖: ");
    tokenStream
            .onPartialResponse(_LOG::info)
            .onCompleteResponse(token -> {
                _LOG.info("\n💬: Do you remember what is my name?\n");
                _LOG.info("🤖: ");
                assistant.chat("Do you remember what is my name?")
                        .onPartialResponse(_LOG::info)
                        .onCompleteResponse(response -> futureChatResponse.complete(response))
                        .onError(Throwable::printStackTrace).start();
            })
            .onError(Throwable::printStackTrace).start();
    futureChatResponse.join();

snippet java-15:
  name: "LangChain4j OVHai dependency"
  prefix: "java-15"
  scope: "xml"
  body: |
    <dependency>
      <groupId>dev.langchain4j</groupId>
      <artifactId>langchain4j-ovh-ai</artifactId>
      <version>1.4.0-beta10</version>
    </dependency>

snippet java-16:
  name: "RAG AI Services"
  prefix: "java-16"
  scope: "java"
  body: |
    interface Assistant {
      @SystemMessage("You are Nestor, a virtual assistant. Answer to the question.")
      TokenStream chat(String message);
    }

snippet java-17:
  name: "RAG model"
  prefix: "java-17"
  scope: "java"
  body: |
    MistralAiStreamingChatModel steamingModel = MistralAiStreamingChatModel.builder()
            .apiKey(System.getenv("OVH_AI_ENDPOINTS_ACCESS_TOKEN"))
            .modelName(System.getenv("OVH_AI_ENDPOINTS_MODEL_NAME"))
            .baseUrl(System.getenv("OVH_AI_ENDPOINTS_MODEL_URL"))
            .maxTokens(512)
            .temperature(0.0)
            .logRequests(false)
            .logResponses(false)
            .build();

snippet java-18:
  name: "RAG memory"
  prefix: "java-18"
  scope: "java"
  body: |
    ChatMemory chatMemory = MessageWindowChatMemory.withMaxMessages(10);

snippet java-19:
  name: "RAG chunk"
  prefix: "java-19"
  scope: "java"
  body: |
    DocumentParser documentParser = new TextDocumentParser();
    Document document = loadDocument(
        RAGChatbot.class.getResource("/rag-files/conference-information-talk-01.md").getFile(),
        documentParser);
    DocumentSplitter splitter = DocumentSplitters.recursive(8000, 50);

    List<TextSegment> segments = splitter.split(document);

snippet java-20:
  name: "RAG embedding"
  prefix: "java-20"
  scope: "java"
  body: |
    EmbeddingModel embeddingModel = OvhAiEmbeddingModel.builder()
                    .apiKey(System.getenv("OVH_AI_ENDPOINTS_ACCESS_TOKEN"))
                    .baseUrl(System.getenv("OVH_AI_ENDPOINTS_EMBEDDING_MODEL"))
                    .build();
    List<Embedding> embeddings = embeddingModel.embedAll(segments).content();

snippet java-21:
  name: "RAG embedding store"
  prefix: "java-21"
  scope: "java"
  body: |
    EmbeddingStore<TextSegment> embeddingStore = new InMemoryEmbeddingStore<>();
    embeddingStore.addAll(embeddings, segments);
    ContentRetriever contentRetriever = EmbeddingStoreContentRetriever.builder()
        .embeddingStore(embeddingStore)
        .embeddingModel(embeddingModel)
        .maxResults(3)
        .minScore(0.1)
        .build();

snippet java-22:
  name: "RAG assistant"
  prefix: "java-22"
  scope: "java"
  body: |
    Assistant assistant = AiServices.builder(Assistant.class)
        .streamingChatModel(steamingModel)
        .chatMemory(chatMemory)
        .contentRetriever(contentRetriever)
        .build();

snippet java-23:
  name: "RAG call"
  prefix: "java-23"
  scope: "java"
  body: |
    _LOG.info("💬: What is the program at AI Summit Barcelona?\n");
    TokenStream tokenStream = assistant.chat("What is the program at AI Summit Barcelona?");
    CompletableFuture<ChatResponse> futureChatResponse = new CompletableFuture<>();

    _LOG.info("🤖: ");
    tokenStream
        .onCompleteResponse(response -> futureChatResponse.complete(response))
        .onPartialResponse(_LOG::info)
        .onError(Throwable::printStackTrace).start();
    futureChatResponse.join();

snippet java-24:
  name: "Tool generateImage tool"
  prefix: "java-24"
  scope: "java"
  body: |
    // Define the tool using the @Tool annotation
    @Tool("""
                Tool to create an image with Stable Diffusion XL given a prompt and a negative prompt.
                """)
    void generateImage(@P("Prompt that explains the image") String prompt, @P("Negative prompt that explains what the image must not contains") String negativePrompt) throws IOException, InterruptedException {
        _LOG.info("Prompt: {}", prompt);
        _LOG.info("Negative prompt: {}", negativePrompt);

        // java-25

        // java-26
    }

snippet java-25:
  name: "Tool call SDXL"
  prefix: "java-25"
  scope: "java"
  body: |
    // Call Stable diffusion model with the prompt and negative prompt
    HttpRequest httpRequest = HttpRequest.newBuilder()
            .uri(URI.create(System.getenv("OVH_AI_ENDPOINTS_SD_URL")))
            .POST(HttpRequest.BodyPublishers.ofString("""
                        {"prompt": "%s", 
                          "negative_prompt": "%s"}
                        """.formatted(prompt, negativePrompt)))
            .header("accept", "application/octet-stream")
            .header("Content-Type", "application/json")
            .header("Authorization", "Bearer " + System.getenv("OVH_AI_ENDPOINTS_ACCESS_TOKEN"))
            .build();

snippet java-26:
  name: "Tool: save image"
  prefix: "java-26"
  scope: "java"
  body: |
    // Create the image file from Stable Diffusion response
    HttpResponse<byte[]> response = HttpClient.newHttpClient()
            .send(httpRequest, HttpResponse.BodyHandlers.ofByteArray());

    _LOG.debug("SDXL status code: {}", response.statusCode());
    Files.write(Path.of("generated-image.jpeg"), response.body());

snippet java-27:
  name: "Tool: AI Services"
  prefix: "java-27"
  scope: "java"
  body: |
    // Create a detailed system prompt: the goal and what the model must generate and use
    @SystemMessage("""
            Your are an expert of using the Stable Diffusion XL model.
            The user explains in natural language what kind of image he wants.
            You must do the following steps:
              - Understand the user's request.
              - Generate the two kinds of prompts for stable diffusion: the prompt and the negative prompt
              - the prompts must be in english and detailed and optimized for the Stable Diffusion XL model. 
              - once and only once you have this two prompts call the tool with the two prompts.
            If asked about to create an image, you MUST call the `generateImage` function.
            """)
    @UserMessage("Create an image with stable diffusion XL following this description: {{userMessage}}")
    String chat(@V("userMessage") String userMessage);

snippet java-28:
  name: "Tool: model selection"
  prefix: "java-28"
  scope: "java"
  body: |
    // Main chatbot configuration, try to be more deterministic as possible ;)
    MistralAiChatModel chatModel = MistralAiChatModel.builder()
            .apiKey(System.getenv("OVH_AI_ENDPOINTS_ACCESS_TOKEN"))
            .baseUrl(System.getenv("OVH_AI_ENDPOINTS_MODEL_URL"))
            .modelName(System.getenv("OVH_AI_ENDPOINTS_MODEL_NAME"))
            .logRequests(false)
            .logResponses(false)
            // To have more deterministic outputs, set temperature to 0.
            .temperature(0.0)
            .build();

snippet java-29:
  name: "Tool: memory"
  prefix: "java-29"
  scope: "java"
  body: |
    // Add memory to fine tune the SDXL prompt.
    ChatMemory chatMemory = MessageWindowChatMemory.withMaxMessages(10);

snippet java-30:
  name: "Tool: build chatbot"
  prefix: "java-30"
  scope: "java"
  body: |
    // Build the chatbot thanks to LangChain4J AI Services mode (see https://docs.langchain4j.dev/tutorials/ai-services)
    ChatBot chatBot = AiServices.builder(ChatBot.class)
            .chatModel(chatModel)
            .tools(new ImageGenTools())
            .chatMemory(chatMemory)
            .build();

snippet java-31:
  name: "Tool: call LLM"
  prefix: "java-31"
  scope: "java"
  body: |
    // Start the conversation loop (enter "exit" to quit)
    String userInput = "";
    Scanner scanner = new Scanner(System.in);
    while (true) {
        _LOG.info("\nEnter your message: ");
        userInput = scanner.nextLine();
        if (userInput.equalsIgnoreCase("exit")) break;
        _LOG.info("\nResponse: " + chatBot.chat(userInput));
    }
    scanner.close();

snippet java-32:
  name: "MCP: dependency"
  prefix: "java-32"
  scope: "xml"
  body: |
    <dependency>
      <groupId>dev.langchain4j</groupId>
      <artifactId>langchain4j-mcp</artifactId>
      <version>1.4.0-beta10</version>
    </dependency>

snippet java-33:
  name: "MCP: AI Services"
  prefix: "java-33"
  scope: "java"
  body: |
    @SystemMessage("""
            Your are an expert of using the Stable Diffusion XL model.
            The user explains in natural language what kind of image he wants.
            You must do the following steps:
              - Understand the user's request.
              - Generate the two kinds of prompts for stable diffusion: the prompt and the negative prompt
              - the prompts must be in english and detailed and optimized for the Stable Diffusion XL model. 
              - once and only once you have this two prompts call the tool with the two prompts.
            If asked about to create an image, you MUST call the `generateImage` function.
            """)
    @UserMessage("Create an image with stable diffusion XL following this description: {{userMessage}}")
    String chat(@V("userMessage") String userMessage);

snippet java-34:
  name: "MCP: model selection"
  prefix: "java-34"
  scope: "java"
  body: |
    MistralAiChatModel chatModel = MistralAiChatModel.builder()
            .apiKey(System.getenv("OVH_AI_ENDPOINTS_ACCESS_TOKEN"))
            .baseUrl(System.getenv("OVH_AI_ENDPOINTS_MODEL_URL"))
            .modelName(System.getenv("OVH_AI_ENDPOINTS_MODEL_NAME"))
            .logRequests(false)
            .logResponses(false)
            // To have more deterministic outputs, set temperature to 0.
            .temperature(0.0)
            .build();

snippet java-35:
  name: "MCP: transport"
  prefix: "java-35-mcp-transport"
  scope: "java"
  body: |
    McpTransport transport = new StreamableHttpMcpTransport.Builder()
            // https://xxxx/mcp/sse
            .url(System.getenv("MCP_SERVER_URL"))
            .logRequests(true)
            .logResponses(true)
            .build();

snippet java-36:
  name: "MCP: mcp client"
  prefix: "java-36-mcp-client"
  scope: "java"
  body: |
    McpClient mcpClient = new DefaultMcpClient.Builder()
            .transport(transport)
            .build();

snippet java-37:
  name: "MCP: tool provider"
  prefix: "java-37-mcp-tool"
  scope: "java"
  body: |
    McpToolProvider toolProvider = McpToolProvider.builder()
            .mcpClients(mcpClient)
            .build();

snippet java-38:
  name: "MCP: memory"
  prefix: "java-38-memory"
  scope: "java"
  body: |
    ChatMemory chatMemory = MessageWindowChatMemory.withMaxMessages(10);

snippet java-39:
  name: "MCP: chatbot"
  prefix: "java-39-chatbot"
  scope: "java"
  body: |
    ChatBot bot = AiServices.builder(ChatBot.class)
            .chatModel(chatModel)
            .toolProvider(toolProvider)
            .chatMemory(chatMemory)
            .build();

snippet java-40:
  name: "MCP: call LLM"
  prefix: "java-40-prompt"
  scope: "java"
  body: |
    String userInput = "";
    Scanner scanner = new Scanner(System.in);
    while (true) {
        _LOG.info("\nEnter your message: ");
        userInput = scanner.nextLine();
        if (userInput.equalsIgnoreCase("exit")) break;
        _LOG.info("\nResponse: " + bot.chat(userInput));
    }
    scanner.close();
    
## java-quarkus section
snippet quarkus-01:
  name: "Quarkus properties"
  prefix: "quarkus-01"
  body: |
    quarkus.langchain4j.mistralai.base-url=\${OVH_AI_ENDPOINTS_MODEL_URL}
    quarkus.langchain4j.mistralai.api-key=\${OVH_AI_ENDPOINTS_ACCESS_TOKEN}
    quarkus.langchain4j.mistralai.chat-model.max-tokens=512
    quarkus.langchain4j.mistralai.chat-model.model-name=\${OVH_AI_ENDPOINTS_MODEL_NAME}
    quarkus.langchain4j.mistralai.log-requests=true
    quarkus.langchain4j.mistralai.log-responses=false
    quarkus.langchain4j.mistralai.chat-model.temperature=0.2

snippet quarkus-02:
  name: "Class annotation simple"
  prefix: "quarkus-02"
  scope: "java"
  body: |
    @RegisterAiService

snippet quarkus-03:
  name: "Entry point to llm simple"
  prefix: "quarkus-03"
  scope: "java"
  body: |
    @SystemMessage("You are a virtual assistant and your name is Nestor.")
    @UserMessage("Answer as best possible to the following question: {question}. The answer must be in a style of a virtual assistant.")
    String askAQuestion(String question);

snippet quarkus-04:
  name: "Class annotation simple resource"
  prefix: "quarkus-04"
  scope: "java"
  body: |
    @Path("/chatbot")

snippet quarkus-05:
  name: "Injection simple resource"
  prefix: "quarkus-05"
  scope: "java"
  body: |
    @Inject
    AISimpleService aiEndpointService;

snippet quarkus-06:
  name: "Simple resource ask method"
  prefix: "quarkus-06"
  scope: "java"
  body: |
    @Path("simple")
    @POST
    public String ask(String question) {
      // Call the askAQuestion method of the AISimpleService service
      return aiEndpointService.askAQuestion(question);
    }
    
snippet quarkus-07:
  name: "Class annotation advanced" 
  prefix: "quarkus-07"
  scope: "java"
  body: |
    @RegisterAiService

snippet quarkus-08:
  name: "Entry point to llm advanced"
  prefix: "quarkus-08"
  scope: "java"
  body: |
    @SystemMessage("You are a virtual assistant and your name is Nestor.")
    @UserMessage("Answer as best possible to the following question: {question}. The answer must be in a style of a virtual assistant.")
    Multi<String> askAQuestion(String question);

snippet quarkus-09:
  name: "Class annotation advanced resource"
  prefix: "quarkus-09"
  scope: "java"
  body: |
    @Path("/chatbot")

snippet quarkus-10:
  name: "Injection advanced resource"
  prefix: "quarkus-10"
  scope: "java"
  body: |
    @Inject
    AIAdvancedService advancedService;

snippet quarkus-11:
  name: "Advanced resource ask method"
  prefix: "quarkus-11"
  scope: "java"
  body: |
    @Path("advanced")
    @POST
    public Multi<String> ask(String question) {
      // Call the askAQuestion method of the AISimpleService service and stream the
      // answer, see https://quarkus.io/guides/getting-started-reactive
      return advancedService.askAQuestion(question);
    }

snippet quarkus-12:
  name: "Class annotation memory"
  prefix: "quarkus-12"
  scope: "java"
  body: |
    @ApplicationScoped
    @RegisterAiService

snippet quarkus-13:
  name: "Entry point to llm memory"
  prefix: "quarkus-13"
  scope: "java"
  body: |
    @SystemMessage("You are a virtual assistant and your name is Nestor.")
    @UserMessage("Answer as best possible to the following question: {question}. The answer must be in a style of a virtual assistant.")
    Multi<String> askAQuestion(String question, @MemoryId String memoryId);

snippet quarkus-14:
  name: "Class annotation memory resource"
  prefix: "quarkus-14"
  scope: "java"
  body: |
    @Path("/chatbot")

snippet quarkus-15:
  name: "Injection memory resource"
  prefix: "quarkus-15"
  scope: "java"
  body: |
    @Inject
    AIMemoryService aiMemoryService;

snippet quarkus-16:
  name: "Memory resource ask method"
  prefix: "quarkus-16"
  scope: "java"
  body: |
    @Path("memory")
    @POST
    public Multi<String> ask(String question) {
      // Call the askAQuestion method of the AISimpleService service and stream the
      // answer, see https://quarkus.io/guides/getting-started-reactive
      return aiMemoryService.askAQuestion(question, "user-one");
    }

snippet quarkus-17:
  name: "quarkus-17-dep-mcp"
  prefix: "quarkus-17"
  scope: "xml"
  body: |
    <dependency>
        <groupId>io.quarkiverse.mcp</groupId>
        <artifactId>quarkus-mcp-server-sse</artifactId>
        <version>1.2.1</version>
    </dependency>

snippet quarkus-18:
  name: "quarkus-18-prop-mcp"
  prefix: "quarkus-18"
  body: |
    # MCP configuration
    quarkus.rest-client."com.ovhcloud.ai.quarkus.chatbot.service.StableDiffusionService".url=\${OVH_AI_ENDPOINTS_SD_URL}

snippet quarkus-19:
  name: "quarkus-19-svc-sdxl"
  prefix: "quarkus-19"
  scope: "java"
  body: |
    @RegisterRestClient
    @ClientHeaderParam(name = "Content-Type", value = "application/json")

snippet quarkus-20:
  name: "quarkus-20-call-sdxl"
  prefix: "quarkus-20"
  scope: "java"
  body: |
    @POST
    @ClientHeaderParam(name = "Authorization", value = "Bearer ${quarkus.langchain4j.mistralai.api-key}")
    byte[] generateImage(SDPayload payload);

snippet quarkus-21:
  name: "quarkus-21-inject-sdxl"
  prefix: "quarkus-21"
  scope: "java"
  body: |
    @RestClient
    StableDiffusionService stableDiffusionService;

snippet quarkus-22:
  name: "quarkus-22-tool"
  prefix: "quarkus-22"
  scope: "java"
  body: |
    // Define the tool using the @Tool annotation
    @Tool(description = "Tool to create an image with Stable Diffusion XL given a prompt and a negative prompt.")
    String generateImage(@P("Prompt that explains the image") String prompt, @P("Negative prompt that explains what the image must not contains") String negativePrompt) throws IOException, InterruptedException {
        _LOG.info("Prompt: {}", prompt);
        _LOG.info("Negative prompt: {}", negativePrompt);

        // quarkus-23
        return "Image generated";
    }

snippet quarkus-23:
  name: "quarkus-23-gen-img"
  prefix: "quarkus-23"
  scope: "java"
  body: |
    byte[] image = stableDiffusionService.generateImage(new SDPayload(prompt, negativePrompt));

    Files.write(Path.of("generated-image.jpeg"), image);

##### Python section
snippet py-33:
  name: "Chatbot requirements"
  prefix: "py-33"
  body: | 
    # LangChain dependencies
    langchain==0.3.27
    langgraph==0.3.27
    langchain-mistralai==0.2.11
    langchain_chroma==0.2.5
    langchain-community==0.3.27
    langchain-mcp-adapters==0.1.9

snippet py-34:
  name: "Chatbot simple model"
  prefix: "py-34"
  scope: "python"
  body: | 
    model = ChatMistralAI(
        base_url=os.getenv("OVH_AI_ENDPOINTS_MODEL_URL"),
        api_key=os.getenv("OVH_AI_ENDPOINTS_ACCESS_TOKEN"),
        model=os.getenv("OVH_AI_ENDPOINTS_MODEL_NAME"),
        max_tokens=512,
        temperature=0
    )

snippet py-35:
  name: "Chatbot simple messages"
  prefix: "py-35"
  scope: "python"
  body: | 
    messages = [
        (
            "system",
            "You are Nestor, a virtual assistant. Answer to the question.",
        ),
        ("human", "Tell me a joke about Python developers,"),
    ]

snippet py-36:
  name: "Chatbot simple model call"
  prefix: "py-36"
  scope: "python"
  body: | 
    ai_msg = model.invoke(messages)

snippet py-37:
  name: "Chatbot simple print response"
  prefix: "py-37"
  scope: "python"
  body: | 
    print("💬: Question: Tell me a joke about Python developers\n")
    print(f"🤖: {ai_msg.content}")

snippet py-38:
  name: "Chatbot stream model"
  prefix: "py-38"
  scope: "python"
  body: | 
    model = ChatMistralAI(
        base_url=os.getenv("OVH_AI_ENDPOINTS_MODEL_URL"),
        api_key=os.getenv("OVH_AI_ENDPOINTS_ACCESS_TOKEN"),
        model=os.getenv("OVH_AI_ENDPOINTS_MODEL_NAME"),
        max_tokens=512,
        temperature=0
    )

snippet py-39:
  name: "Chatbot stream messages"
  prefix: "py-39"
  scope: "python"
  body: | 
    messages = [
        (
            "system",
            "You are Nestor, a virtual assistant. Answer to the question.",
        ),
        ("human", "Tell me a joke about Python developers"),
    ]

snippet py-40:
  name: "Chatbot stream print"
  prefix: "py-40"
  scope: "python"
  body: | 
    print("💬: Question: Tell me a joke about Python developers\n")
    print("🤖:")

snippet py-41:
  name: "Chatbot stream call model"
  prefix: "py-41"
  scope: "python"
  body: | 
    for chunk in model.stream(messages):
      print(chunk.content, end="", flush=True)

snippet py-42:
  name: "Chatbot memory model"
  prefix: "py-42"
  scope: "python"
  body: | 
    model = ChatMistralAI(
        base_url=os.getenv("OVH_AI_ENDPOINTS_MODEL_URL"),
        api_key=os.getenv("OVH_AI_ENDPOINTS_ACCESS_TOKEN"),
        model=os.getenv("OVH_AI_ENDPOINTS_MODEL_NAME"),
        max_tokens=512,
        temperature=0
    )

snippet py-43:
  name: "Chatbot memory LangGraph creation"
  prefix: "py-43"
  scope: "python"
  body: | 
    workflow = StateGraph(state_schema=MessagesState)

snippet py-44:
  name: "Chatbot memory function call"
  prefix: "py-44"
  scope: "python"
  body: | 
    def call_model(state: MessagesState):
      response = model.invoke(
          [SystemMessage("You are Nestor, a virtual assistant. Answer to the question.")] 
          + state["messages"]
      )

      return {"messages": response.content}

snippet py-45:
  name: "Chatbot memory nodes"
  prefix: "py-45"
  scope: "python"
  body: | 
    workflow.add_edge(START, "model")
    workflow.add_node("model", call_model)

snippet py-46:
  name: "Chatbot memory add memory"
  prefix: "py-46"
  scope: "python"
  body: | 
    memory = MemorySaver()

snippet py-47:
  name: "Chatbot memory graph compile"
  prefix: "py-47"
  scope: "python"
  body: | 
    app = workflow.compile(
      checkpointer=memory
    )

snippet py-48:
  name: "Chatbot memory thread ID"
  prefix: "py-48"
  scope: "python"
  body: | 
    thread_id = uuid.uuid4()
    config = {"configurable": {"thread_id": thread_id}}

snippet py-49:
  name: "Chatbot memory model call"
  prefix: "py-49"
  scope: "python"
  body: | 
    question = "Hello, my name is Stéphane"
    print(f"👤: {question}")
    print("🤖: ")
    for message_chunk, metadata in app.stream( 
        {"messages": "Hello, my name is Stéphane"}, config,
        stream_mode="messages",
    ):
        if message_chunk.content:
            print(message_chunk.content, end="", flush=True)

    question = "Do you remember my name?"
    print(f"👤: {question}")
    print("🤖: ")
    for message_chunk, metadata in app.stream( 
        {"messages": "Do you remember my name?"}, config,
        stream_mode="messages",
    ):
        if message_chunk.content:
            print(message_chunk.content, end="", flush=True)

snippet py-50:
  name: "Chatbot RAG model"
  prefix: "py-50"
  scope: "python"
  body: | 
    model = ChatMistralAI(
    base_url=os.getenv("OVH_AI_ENDPOINTS_MODEL_URL"),
    api_key=os.getenv("OVH_AI_ENDPOINTS_ACCESS_TOKEN"),
    model=os.getenv("OVH_AI_ENDPOINTS_MODEL_NAME"),
    max_tokens=512,
    temperature=0
    )

snippet py-51:
  name: "Chatbot RAG doc loading"
  prefix: "py-51"
  scope: "python"
  body: | 
    loader = TextLoader("./rag-files/conference-information-talk-01.md")
    docs = loader.load()

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    splits = text_splitter.split_documents(docs)

snippet py-52:
  name: "Chatbot RAG vector DB"
  prefix: "py-52"
  scope: "python"
  body: | 
    vectorstore = Chroma.from_documents(documents=splits, 
                   embedding=OVHCloudEmbeddings(model_name="bge-multilingual-gemma2", 
                   access_token=os.getenv("OVH_AI_ENDPOINTS_ACCESS_TOKEN")))

snippet py-53:
  name: "Chatbot RAG prompt"
  prefix: "py-53"
  scope: "python"
  body: | 
    prompt = hub.pull("rlm/rag-prompt")

snippet py-54:
  name: "Chatbot RAG chain"
  prefix: "py-54"
  scope: "python"
  body: | 
    rag_chain = (
      {"context": vectorstore.as_retriever(), "question": RunnablePassthrough()} |
      prompt
      | model
    )

snippet py-55:
  name: "Chatbot RAG print"
  prefix: "py-55"
  scope: "python"
  body: | 
    print("👤: What is the program at AI Summit Barcelona?")
    print("🤖:")
    for r in rag_chain.stream({"question", "What is the program at AI Summit Barcelona?"}):
      print(r.content, end="", flush=True)
      time.sleep(0.150)

snippet py-56:
  name: "Image gen model"
  prefix: "py-56"
  scope: "python"
  body: | 
    model = ChatMistralAI(
      base_url=os.getenv("OVH_AI_ENDPOINTS_MODEL_URL"),
      api_key=os.getenv("OVH_AI_ENDPOINTS_ACCESS_TOKEN"),
      model=os.getenv("OVH_AI_ENDPOINTS_MODEL_NAME"),
      temperature=0
    )

snippet py-57:
  name: "Image gen tool"
  prefix: "py-57"
  scope: "python"
  body: | 
    @tool
    def generateImage(prompt: str, negative_prompt: str) -> str:
        # Give a detailed description of the tool to "help" the model
        # py-58

        print("⚡️ Generate image with Stable Diffusion XL 🏞️")

        # Payload and headers to send to SDXL API
        # py-59        

        # SDXL call and image save on the file system
        file_path = "./generated-image.png"
        
        # py-60

        return f"🖼️ Image generated: {file_path}"

snippet py-58:
  name: "Image gen tool description"
  prefix: "py-58"
  scope: "python"
  body: | 
    """Tool to create an image with Stable Diffusion XL given a prompt and a negative prompt.
      - prompt: Prompt that explains the image
      - negative_prompt: Negative prompt that explains what the image must not contains
    """

snippet py-59:
  name: "Image gen tool payload"
  prefix: "py-59"
  scope: "python"
  body: | 
    data = {
        "prompt": prompt,
        "negative_prompt": negative_prompt
    }

    headers = {
        "accept": "application/octet-stream",
        "content-type": "application/json",
        "Authorization": f"Bearer {os.getenv('OVH_AI_ENDPOINTS_ACCESS_TOKEN')}",
    }

snippet py-60:
  name: "Image gen tool request"
  prefix: "py-60"
  scope: "python"
  body: | 
    response = requests.post(os.getenv("OVH_AI_ENDPOINTS_SD_URL"), headers=headers, data=json.dumps(data))
    if response.status_code == 200:
        # Handle response
        response_data = response.content
        with open(file_path, 'wb') as file:
          file.write(response_data)
    else:
        print("Error:", response.status_code)
        return f"❌ Unable to generate image ({file_path}): {response.status_code} ❌"

snippet py-61:
  name: "Image gen model tools"
  prefix: "py-61"
  scope: "python"
  body: | 
    tools = [generateImage]

    model_with_tools = model.bind_tools(tools)

snippet py-62:
  name: "Image gen messages"
  prefix: "py-62"
  scope: "python"
  body: | 
    question = "A red cat on a couch"
    messages = [HumanMessage(role="user", content=question), 
            SystemMessage(role="system", content="""
        Your are an expert of using the Stable Diffusion XL model.
        The user explains in natural language what kind of image he wants.
        You must do the following steps:
          - Understand the user's request.
          - Generate the two kinds of prompts for stable diffusion: the prompt and the negative prompt
          - the prompts must be in english and detailed and optimized for the Stable Diffusion XL model. 
          - once and only once you have this two prompts call the tool with the two prompts.
        If asked about to create an image, you MUST call the `generateImage` function.
        """)]

snippet py-63:
  name: "Image gen first model call"
  prefix: "py-63"
  scope: "python"
  body: | 
    print(f"💬: {question}")

    ai_msg = model_with_tools.invoke(messages)
    print(f"🤖: {ai_msg.content}")

    messages.append({
           "role": "assistant",
           "type": "message",
           "content": ai_msg.content
       })

snippet py-64:
  name: "Image gen tool call"
  prefix: "py-64"
  scope: "python"
  body: | 
    for tool_call in ai_msg.tool_calls:
        selected_tool = {"generateImage": generateImage}[tool_call["name"]]
        tool_result = selected_tool.invoke(tool_call)
        messages.append(tool_result)

snippet py-65:
  name: "Image gen final model call"
  prefix: "py-65"
  scope: "python"
  body: | 
    ai_msg = model_with_tools.invoke(messages)

snippet py-66:
  name: "Image gen print final response"
  prefix: "py-66"
  scope: "python"
  body: | 
    print(f"🤖: {ai_msg.content}")

snippet py-67:
  name: "MCP server tool"
  prefix: "py-67"
  scope: "python"
  body: | 
    # Give a detailed description of the tool to "help" the model
    """Tool to create an image with Stable Diffusion XL given a prompt and a negative prompt.
      - prompt: Prompt that explains the image
      - negative_prompt: Negative prompt that explains what the image must not contains
    """
    print("⚡️ Generate image with Stable Diffusion XL 🏞️")

    # Payload and headers to send to SDXL API
    data = {
        "prompt": prompt,
        "negative_prompt": negative_prompt
    }

    headers = {
        "accept": "application/octet-stream",
        "content-type": "application/json",
        "Authorization": f"Bearer {os.getenv('OVH_AI_ENDPOINTS_ACCESS_TOKEN')}",
    }

    # SDXL call and image save on the file system
    file_path = "./generated-image-mcp.png"

    response = requests.post(os.getenv("OVH_AI_ENDPOINTS_SD_URL"), headers=headers, data=json.dumps(data))
    if response.status_code == 200:
        # Handle response
        response_data = response.content
        with open(file_path, 'wb') as file:
          file.write(response_data)
    else:
        print("Error:", response.status_code)
        return f"❌ Unable to generate image ({file_path}): {response.status_code} ❌"

    return f"🖼️ Image generated: {file_path}"

snippet py-68:
  name: "MCP client client"
  prefix: "py-68"
  scope: "python"
  body: | 
    client = MultiServerMCPClient(
        {
            "gen_images": {
                "transport": "streamable_http",
                "url": "http://localhost:8000/mcp/"
            },
        }
    )

snippet py-69:
  name: "MCP client model"
  prefix: "py-69"
  scope: "python"
  body: | 
    model = ChatMistralAI(
        base_url=os.getenv("OVH_AI_ENDPOINTS_MODEL_URL"),
        api_key=os.getenv("OVH_AI_ENDPOINTS_ACCESS_TOKEN"),
        model=os.getenv("OVH_AI_ENDPOINTS_MODEL_NAME"),
        temperature=0
    )

snippet py-70:
  name: "MCP client configure tools"
  prefix: "py-70"
  scope: "python"
  body: |
    tools = await client.get_tools()
    model_with_tools = model.bind_tools(tools)

snippet py-71:
  name: "MCP client messages"
  prefix: "py-71"
  scope: "python"
  body: |
    question = "A red cat with green eyes on a couch"
    messages = [HumanMessage(role="user", content=question), 
                SystemMessage(role="system", content="""
            Your are an expert of using the Stable Diffusion XL model.
            The user explains in natural language what kind of image he wants.
            You must do the following steps:
              - Understand the user's request.
              - Generate the two kinds of prompts for stable diffusion: the prompt and the negative prompt
              - the prompts must be in english and detailed and optimized for the Stable Diffusion XL model. 
              - once and only once you have this two prompts call the tool with the two prompts.
            If asked about to create an image, you MUST call the `generateImage` function.
            """)]

snippet py-72:
  name: "MCP client first model call"
  prefix: "py-72"
  scope: "python"
  body: |
    print(f"💬: {question}")

    ai_msg = await model_with_tools.ainvoke(messages)
    messages.append({
          "role": "assistant",
          "type": "message",
          "content": ai_msg.content
      })

snippet py-73:
  name: "MCP client tool calling"
  prefix: "py-73"
  scope: "python"
  body: |
    for tool_call in ai_msg.tool_calls:
        selected_tool = {"generateImage": tools}[tool_call["name"]]
        tool_result = await selected_tool[0].ainvoke(tool_call)
        messages.append(tool_result)

snippet py-74:
  name: "MCP client last model call"
  prefix: "py-74"
  scope: "python"
  body: |
    ai_msg = model_with_tools.invoke(messages)

snippet py-75:
  name: "MCP client print response"
  prefix: "py-75"
  scope: "python"
  body: |
    print(f"🤖:{ai_msg.content}")