# Audio section
snippet audio-requirements:
  name: "Audio requirements"
  prefix: "py-01"
  #scope: "python"
  body: | 
    # audio section
    # Gradio to create the application, see https://pypi.org/project/gradio/
    gradio==5.31.0
    # Pydub to manipulate audio files, see https://pypi.org/project/pydub/
    pydub==0.25.1
    # Numpy to do sientific computing with Python
    numpy==2.1.3
    # Manage http requests, see https://pypi.org/project/requests/
    requests==2.32.3
    # OpenAI client for Whisper
    openai==1.97.0

# Speech to text HTTP section
snippet py-02:
  name: "STT client initialization"
  prefix: "py-02"
  scope: "python"
  body: | 
    client = OpenAI(base_url=os.environ.get('OVH_AI_ENDPOINTS_WHISPER_URL'), 
                api_key=os.environ.get('OVH_AI_ENDPOINTS_ACCESS_TOKEN'))

snippet py-03:
  name: "STT Whisper call"
  prefix: "py-03"
  scope: "python"
  body: | 
    with open(audio, "rb") as audio_file:
        # Call Whisper transcription API
        transcript = client.audio.transcriptions.create(
            model=os.environ.get('OVH_AI_ENDPOINTS_WHISPER_MODEL'),
            file=audio_file,
            temperature=0.0,
            response_format="text"
        )

snippet py-04:
  name: "STT return transcript"
  prefix: "py-04"
  scope: "python"
  body: | 
    return transcript

snippet py-05:
  name: "STT input-audio"
  prefix: "py-05"
  scope: "python"
  body: | 
    input_audio = gr.Audio(
        sources=['upload', 'microphone'],
        type='filepath',
        label="üéôÔ∏è"
    )

snippet py-06:
  name: "STT Gradio interface"
  prefix: "py-06"
  scope: "python"
  body: | 
    demo = gr.Interface(
        fn=speechToText,
        inputs=input_audio,
        outputs=gr.Textbox(lines=10, label="Transcription"),
        allow_flagging="never"
    )

# Text to speech HTTP section
snippet py-08:
  name: "TTS model URL"
  prefix: "py-08"
  scope: "python"
  body: | 
    url = "https://nvr-tts-en-us.endpoints.kepler.ai.cloud.ovh.net/api/v1/tts/text_to_audio"

snippet py-09:
  name: "TTS bearer"
  prefix: "py-09"
  scope: "python"
  body: | 
    headers = {
        "Authorization": f"Bearer {os.getenv('OVH_AI_ENDPOINTS_ACCESS_TOKEN')}",
    }

snippet py-10:
  name: "TTS HTTP payload"
  prefix: "py-10"
  scope: "python"
  body: | 
    payload = {
        "encoding": 1,
        "language_code": "en-US",
        "sample_rate_hz": 16000,
        "text": textToTransform,
        "voice_name": "English-US.Female-1",
    }

snippet py-11:
  name: "TTS HTTP response"
  prefix: "py-11"
  scope: "python"
  body: | 
    response = requests.post(url, json=payload, headers=headers)

snippet py-12:
  name: "TTS HTTP return"
  prefix: "py-12"
  scope: "python"
  body: | 
    audio = (16000, np.frombuffer(response.content, dtype=np.int16))

    return audio

snippet py-13:
  name: "TTS HTTP input"
  prefix: "py-13"
  scope: "python"
  body: | 
    input_text = gr.Textbox(
        label="üè¥Û†ÅßÛ†Å¢Û†Å•Û†ÅÆÛ†ÅßÛ†Åø",
    )

snippet py-14:
  name: "TTS HTTP output"
  prefix: "py-14"
  scope: "python"
  body: | 
    output_audio = gr.Audio(
        label="Audio synthesis (.wav)", type="numpy", show_download_button=False
    )

snippet py-15:
  name: "TTS HTTP demo"
  prefix: "py-15"
  scope: "python"
  body: | 
    demo = gr.Interface(
        fn=text_to_speech, inputs=input_text, outputs=output_audio, allow_flagging="never"
    )


# Speech to speech
snippet py-16:
  name: "STS OpenAI client configuration"
  prefix: "py-16"
  scope: "python"
  body: | 
    client = OpenAI(base_url=os.environ.get('OVH_AI_ENDPOINTS_WHISPER_URL'), 
            api_key=os.environ.get('OVH_AI_ENDPOINTS_ACCESS_TOKEN'))


snippet py-17:
  name: "STS transcript with Whisper"
  prefix: "py-17"
  scope: "python"
  body: | 
    with open(audio, "rb") as audio_file:
        # Call Whisper transcription API
        transcript = client.audio.transcriptions.create(
            model=os.environ.get('OVH_AI_ENDPOINTS_WHISPER_MODEL'),
            file=audio_file,
            temperature=0.0,
            response_format="text"
        )

snippet py-18:
  name: "STS return transcript"
  prefix: "py-18"
  scope: "python"
  body: | 
    return transcript

snippet py-19:
  name: "STS LLM configuration"
  prefix: "py-19"
  scope: "python"
  body: | 
    url = f"{os.getenv('OVH_AI_ENDPOINTS_MODEL_URL')}/chat/completions"
    payload = {
        "max_tokens": 512,
        "messages": [
            {
                "content": "Do not add any other words or explanations than the translation requested.",
                "role": "system"
            },
            {
                "content": "Translate the phrase following sentence in Spanish: " + english_text,
                "role": "user"
            }
        ],
        "model": f"{os.getenv('OVH_AI_ENDPOINTS_MODEL_NAME')}",
        "temperature": 0,
    }

snippet py-20:
  name: "STT LLM bearer"
  prefix: "py-20"
  scope: "python"
  body: | 
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {os.getenv('OVH_AI_ENDPOINTS_ACCESS_TOKEN')}",
    }

snippet py-21:
  name: "STT LLM call"
  prefix: "py-21"
  scope: "python"
  body: | 
    # Send the request to endpoint with text to transform
    response = requests.post(url, json=payload, headers=headers)

snippet py-22:
  name: "STT translation return"
  prefix: "py-22"
  scope: "python"
  body: | 
    # Send the request to endpoint with text to transform
    response = requests.post(url, json=payload, headers=headers)

    # Return the translation
    text = "‚ùå Noting to translate ‚ùå"
    if response.status_code == 200:
        # Handle response
        response_data = response.json()
        # Parse JSON response
        choices = response_data["choices"]
        for choice in choices:
            text = choice["message"]["content"]
            # Process text and finish_reason
            print(text)
    else:
        print("Error:", response.status_code)
    
    return text

snippet py-23:
  name: "STS RIVA Spanish model URL"
  prefix: "py-23"
  scope: "python"
  body: | 
    url = os.environ.get('OVH_AI_ENDPOINTS_TTS_MODEL')

snippet py-24:
  name: "STS RIVA bearer"
  prefix: "py-24"
  scope: "python"
  body: | 
    headers = {
        "Authorization": f"Bearer {os.getenv('OVH_AI_ENDPOINTS_ACCESS_TOKEN')}",
    }

snippet py-25:
  name: "STS RIVA payload"
  prefix: "py-25"
  scope: "python"
  body: | 
    payload = {
        "encoding": 1,
        "language_code": "es-ES",
        "sample_rate_hz": 16000,
        "text": textToTransform,
        "voice_name": "Spanish-ES-Female-1",
    }

snippet py-26:
  name: "STS RIVA call"
  prefix: "py-26"
  scope: "python"
  body: | 
    response = requests.post(url, json=payload, headers=headers)

    # Return the audio
    # see https://numpy.org/doc/stable/reference/generated/numpy.frombuffer.html
    audio = (16000, np.frombuffer(response.content, dtype=np.int16))

    return audio

snippet py-27:
  name: "STS call English STT"
  prefix: "py-27"
  scope: "python"
  body: | 
    englishText = speechToText(audio)


snippet py-28:
  name: "STS call translation"
  prefix: "py-28"
  scope: "python"
  body: | 
    spanishText = translate_en_to_spanish(englishText)

snippet py-29:
  name: "STS call Spanish TTS"
  prefix: "py-29"
  scope: "python"
  body: | 
    return [englishText, spanishText, text_to_speech(spanishText)]

snippet py-30:
  name: "STS English audio input"
  prefix: "py-30"
  scope: "python"
  body: | 
    input_audio = gr.Audio(label = "English üè¥Û†ÅßÛ†Å¢Û†Å•Û†ÅÆÛ†ÅßÛ†Åø", sources=["upload", "microphone"], type="filepath")

snippet py-31:
  name: "STS Spanish text output"
  prefix: "py-31"
  scope: "python"
  body: | 
    output_audio = gr.Audio(
      label="Spanish version üá™üá∏", type="numpy", show_download_button=False
    )

snippet py-32:
  name: "STS Gradio interface"
  prefix: "py-32"
  scope: "python"
  body: | 
    demo = gr.Interface(
    fn=speechToSpeech,
    inputs=input_audio,
    outputs=[gr.Textbox(lines=10, label="üè¥Û†ÅßÛ†Å¢Û†Å•Û†ÅÆÛ†ÅßÛ†Åø"), gr.Textbox(lines=10, label="üá™üá∏"), output_audio],
    allow_flagging="never",
    )

snippet java-41:
  name: "OpenAI client initialisation"
  prefix: "java-41"
  scope: "java"
  body: |
    OpenAIClient client = OpenAIOkHttpClient.builder()
            .apiKey(System.getenv("OVH_AI_ENDPOINTS_ACCESS_TOKEN"))
            .baseUrl(System.getenv("OVH_AI_ENDPOINTS_WHISPER_URL"))
            .build();

snippet java-42:
  name: "STT Whisper configuration"
  prefix: "java-42"
  scope: "java"
  body: |
    TranscriptionCreateParams createParams = TranscriptionCreateParams.builder()
            .model(System.getenv("OVH_AI_ENDPOINTS_WHISPER_MODEL"))
            .responseFormat(AudioResponseFormat.TEXT)
            .language("en")
            .file(record)
            .build();

snippet java-43:
  name: "Transcription"
  prefix: "java-43"
  scope: "java"
  body: |
    Transcription transcription =
        client.audio().transcriptions().create(createParams).asTranscription();
    System.out.println("üìù Transcript generated! üìù");
    return transcription.text();

snippet java-44:
  name: "Javelit Ux"
  prefix: "java-44"
  scope: "java"
  body: |
    Jt.title("Speech to text exercise").use();

    var recording = Jt.audioInput("üè¥Û†ÅßÛ†Å¢Û†Å•Û†ÅÆÛ†ÅßÛ†ÅøÛ†ÅßÛ†Å¢English audio üè¥Û†ÅßÛ†Å¢Û†Å•Û†ÅÆÛ†ÅßÛ†Åø").use();

    if (recording != null) {
      var transcription = speechToText(recording.content());
      Jt.text(String.format("""
            üè¥Û†ÅßÛ†Å¢Û†Å•Û†ÅÆÛ†ÅßÛ†Åø Û†ÅßÛ†Å¢Û†Å•Û†ÅÆÛ†ÅßÛ†ÅøÛ†ÅßÛ†Å¢English transcription üè¥Û†ÅßÛ†Å¢Û†Å•Û†ÅÆÛ†ÅßÛ†ÅøÛ†ÅßÛ†Å¢
            %s
            """, transcription))
        .use();
    }

snippet java-45:
  name: "OkHttp Client"
  prefix: "java-45"
  scope: "java"
  body: |
    OkHttpClient client = new OkHttpClient();

snippet java-46:
  name: "Payload"
  prefix: "java-46"
  scope: "java"
  body: |
    String payload = """
            {
              "encoding": 1,
              "language_code": "en-US",
              "sample_rate_hz": 16000,
              "text": "%s",
              "voice_name": "English-US.Female-1"
            }
            """;

snippet java-47:
  name: "RIVA request"
  prefix: "java-47"
  scope: "java"
  body: |
    RequestBody body = RequestBody.create(String.format(payload, "Hello, World!!"), MediaType.get("application/json; charset=utf-8"));
    Request request = new Request.Builder()
            .url("https://nvr-tts-en-us.endpoints.kepler.ai.cloud.ovh.net/api/v1/tts/text_to_audio")
            .addHeader("Authorization", String.format("Bearer %s", System.getenv("OVH_AI_ENDPOINTS_ACCESS_TOKEN")))
            .header("accept", "application/octet-stream")
            .post(body)
            .build();

snippet java-48:
  name: "Call the model"
  prefix: "java-48"
  scope: "java"
  body: |
    System.out.println("‚è≥ Speech creation...");
    Response response = client.newCall(request).execute();
    byte[] audio = response.body().bytes();
    System.out.println("üéµ Speech created üéµ");

    return audio;

snippet java-49:
  name: "Javelit Ux"
  prefix: "java-49"
  scope: "java"
  body: |
    Jt.title("Text to speech exercise").use();
    String englishText = Jt.textArea("üè¥Û†ÅßÛ†Å¢Û†Å•Û†ÅÆÛ†ÅßÛ†ÅøÛ†ÅßÛ†Å¢English text üè¥Û†ÅßÛ†Å¢Û†Å•Û†ÅÆÛ†ÅßÛ†Åø").use();

    if (!englishText.isEmpty()) {
      byte[] textToSpeech = textToSpeech(englishText);

      Jt.audio(textToSpeech)
          .format("audio/wav")
        .use();
    }

snippet java-50:
  name: "OpenAI client configuration"
  prefix: "java-50"
  scope: "java"
  body: |
    OpenAIClient client = OpenAIOkHttpClient.builder()
            .apiKey(System.getenv("OVH_AI_ENDPOINTS_ACCESS_TOKEN"))
            .baseUrl(System.getenv("OVH_AI_ENDPOINTS_WHISPER_URL"))
            .build();

snippet java-51:
  name: "Whisper configuration"
  prefix: "java-51"
  scope: "java"
  body: |
    TranscriptionCreateParams createParams = TranscriptionCreateParams.builder()
                                                                      .model(System.getenv("OVH_AI_ENDPOINTS_WHISPER_MODEL"))
                                                                      .responseFormat(AudioResponseFormat.TEXT)
                                                                      .language("en")
                                                                      .file(record)
                                                                      .build();
snippet java-52:
  name: "Transcription"
  prefix: "java-52"
  scope: "java"
  body: |
    Transcription transcription =
        client.audio().transcriptions().create(createParams).asTranscription();
    System.out.println("üìù Transcript generated! üìù");
    return transcription.text();


snippet java-53:
  name: "OkHttp client"
  prefix: "java-53"
  scope: "java"
  body: |
    OkHttpClient client = new OkHttpClient();

snippet java-54:
  name: "Payload"
  prefix: "java-54"
  scope: "java"
  body: |
    String payload = """
        {
          "encoding": 1,
          "language_code": "es-ES",
          "sample_rate_hz": 16000,
          "text": "%s",
          "voice_name": "Spanish-ES-Female-1"
        }
        """;

snippet java-55:
  name: "RIVA configuration"
  prefix: "java-55"
  scope: "java"
  body: |
    RequestBody body = RequestBody.create(String.format(payload, translatedText), MediaType.get("application/json; charset=utf-8"));
    Request request = new Request.Builder()
        .url("https://nvr-tts-es-es.endpoints.kepler.ai.cloud.ovh.net/api/v1/tts/text_to_audio")
        .addHeader("Authorization", String.format("Bearer %s", System.getenv("OVH_AI_ENDPOINTS_ACCESS_TOKEN")))
        .header("accept", "application/octet-stream")
        .post(body)
        .build();

snippet java-56:
  name: "RIVA call"
  prefix: "java-56"
  scope: "java"
  body: |
    System.out.println("‚è≥ Speech creation...");
    Response response = client.newCall(request).execute();
    byte[] audio = response.body().bytes();
    System.out.println("üéµ Speech created üéµ");

    return audio;

snippet java-57:
  name: "AI Services configuration"
  prefix: "java-57"
  scope: "java"
  body: |
    interface ChatBot {
        @SystemMessage("""
        Do not add any other words or explanations than the translation requested.
        """)
        @UserMessage("Translate the following sentence in Spanish: {{userMessage}}")
        String chat(String userMessage);
    }

snippet java-58:
  name: "Javelit Speech to text"
  prefix: "java-58"
  scope: "java"
  body: |
    var recording = Jt.audioInput("üè¥Û†ÅßÛ†Å¢Û†Å•Û†ÅÆÛ†ÅßÛ†ÅøÛ†ÅßÛ†Å¢English audio üè¥Û†ÅßÛ†Å¢Û†Å•Û†ÅÆÛ†ÅßÛ†Åø").use();
    
    var transcription = "";
    if (recording != null) {
      transcription = speechToText(recording.content());
      Jt.text(String.format("""
            üè¥Û†ÅßÛ†Å¢Û†Å•Û†ÅÆÛ†ÅßÛ†Åø Û†ÅßÛ†Å¢Û†Å•Û†ÅÆÛ†ÅßÛ†ÅøÛ†ÅßÛ†Å¢English transcription üè¥Û†ÅßÛ†Å¢Û†Å•Û†ÅÆÛ†ÅßÛ†ÅøÛ†ÅßÛ†Å¢
            %s
            """, transcription))
        .use();
    }

snippet java-59:
  name: "Configure LLM"
  prefix: "java-59"
  scope: "java"
  body: |
    String translatedText = "";
    if (!transcription.isEmpty()) {
      ChatModel model = OpenAiChatModel.builder()
                                       .apiKey(System.getenv("OVH_AI_ENDPOINTS_ACCESS_TOKEN"))
                                       .modelName(System.getenv("OVH_AI_ENDPOINTS_MODEL_NAME"))
                                       .baseUrl(System.getenv("OVH_AI_ENDPOINTS_MODEL_URL"))
                                       .temperature(0.0)
                                       .logRequests(false)
                                       .logResponses(false)
                                       .build();

      // Call the model to do the translation
      // java-60
    }

snippet java-60:
  name: "translation"
  prefix: "java-60"
  scope: "java"
  body: |
    Jt.text("üîÑ Translating text to Spanish...üîÑ").use();
    ChatBot chatbot = AiServices.create(ChatBot.class, model);
    translatedText = chatbot.chat(transcription);
    Jt.text(String.format("üá™üá∏ Translated text: %s", translatedText)).use();
    
snippet java-61:
  name: "Javelit text to Speech"
  prefix: "java-61"
  scope: "java"
  body: |
    if (!translatedText.isEmpty()) {
      byte[] textToSpeech = textToSpeech(translatedText);

      Jt.audio(textToSpeech)
        .format("audio/wav")
        .use();
    }
