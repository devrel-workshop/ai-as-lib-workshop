# Audio section
snippet audio-requirements:
  name: "Audio requirements"
  prefix: "py-01"
  #scope: "python"
  body: | 
    # audio section
    # Gradio to create the application, see https://pypi.org/project/gradio/
    gradio==5.31.0
    # Pydub to manipulate audio files, see https://pypi.org/project/pydub/
    pydub==0.25.1
    # Numpy to do sientific computing with Python
    numpy==2.1.3
    # Manage http requests, see https://pypi.org/project/requests/
    requests==2.32.3
    # OpenAI client for Whisper
    openai==1.97.0

# Speech to text HTTP section
snippet py-02:
  name: "STT client initialization"
  prefix: "py-02"
  scope: "python"
  body: | 
    client = OpenAI(base_url=os.environ.get('OVH_AI_ENDPOINTS_WHISPER_URL'), 
                api_key=os.environ.get('OVH_AI_ENDPOINTS_ACCESS_TOKEN'))

snippet py-03:
  name: "STT Whisper call"
  prefix: "py-03"
  scope: "python"
  body: | 
    with open(audio, "rb") as audio_file:
        # Call Whisper transcription API
        transcript = client.audio.transcriptions.create(
            model=os.environ.get('OVH_AI_ENDPOINTS_WHISPER_MODEL'),
            file=audio_file,
            temperature=0.0,
            response_format="text"
        )

snippet py-04:
  name: "STT return transcript"
  prefix: "py-04"
  scope: "python"
  body: | 
    return transcript

snippet py-05:
  name: "STT input-audio"
  prefix: "py-05"
  scope: "python"
  body: | 
    input_audio = gr.Audio(
        sources=['upload', 'microphone'],
        type='filepath',
        label="üéôÔ∏è"
    )

snippet py-06:
  name: "STT Gradio interface"
  prefix: "py-06"
  scope: "python"
  body: | 
    demo = gr.Interface(
        fn=speechToText,
        inputs=input_audio,
        outputs=gr.Textbox(lines=10, label="Transcription"),
        allow_flagging="never"
    )

# Text to speech HTTP section
snippet py-08:
  name: "TTS model URL"
  prefix: "py-08"
  scope: "python"
  body: | 
    url = "https://nvr-tts-en-us.endpoints.kepler.ai.cloud.ovh.net/api/v1/tts/text_to_audio"

snippet py-09:
  name: "TTS bearer"
  prefix: "py-09"
  scope: "python"
  body: | 
    headers = {
        "Authorization": f"Bearer {os.getenv('OVH_AI_ENDPOINTS_ACCESS_TOKEN')}",
    }

snippet py-10:
  name: "TTS HTTP payload"
  prefix: "py-10"
  scope: "python"
  body: | 
    payload = {
        "encoding": 1,
        "language_code": "en-US",
        "sample_rate_hz": 16000,
        "text": textToTransform,
        "voice_name": "English-US.Female-1",
    }

snippet py-11:
  name: "TTS HTTP response"
  prefix: "py-11"
  scope: "python"
  body: | 
    response = requests.post(url, json=payload, headers=headers)

snippet py-12:
  name: "TTS HTTP return"
  prefix: "py-12"
  scope: "python"
  body: | 
    audio = (16000, np.frombuffer(response.content, dtype=np.int16))

    return audio

snippet py-13:
  name: "TTS HTTP input"
  prefix: "py-13"
  scope: "python"
  body: | 
    input_text = gr.Textbox(
        label="üè¥Û†ÅßÛ†Å¢Û†Å•Û†ÅÆÛ†ÅßÛ†Åø",
    )

snippet py-14:
  name: "TTS HTTP output"
  prefix: "py-14"
  scope: "python"
  body: | 
    output_audio = gr.Audio(
        label="Audio synthesis (.wav)", type="numpy", show_download_button=False
    )

snippet py-15:
  name: "TTS HTTP demo"
  prefix: "py-15"
  scope: "python"
  body: | 
    demo = gr.Interface(
        fn=text_to_speech, inputs=input_text, outputs=output_audio, allow_flagging="never"
    )


# Speech to speech
snippet py-16:
  name: "STS OpenAI client configuration"
  prefix: "py-16"
  scope: "python"
  body: | 
    client = OpenAI(base_url=os.environ.get('OVH_AI_ENDPOINTS_WHISPER_URL'), 
            api_key=os.environ.get('OVH_AI_ENDPOINTS_ACCESS_TOKEN'))


snippet py-17:
  name: "STS transcript with Whisper"
  prefix: "py-17"
  scope: "python"
  body: | 
    with open(audio, "rb") as audio_file:
        # Call Whisper transcription API
        transcript = client.audio.transcriptions.create(
            model=os.environ.get('OVH_AI_ENDPOINTS_WHISPER_MODEL'),
            file=audio_file,
            temperature=0.0,
            response_format="text"
        )

snippet py-18:
  name: "STS return transcript"
  prefix: "py-18"
  scope: "python"
  body: | 
    return transcript

snippet py-19:
  name: "STS LLM configuration"
  prefix: "py-19"
  scope: "python"
  body: | 
    url = f"{os.getenv('OVH_AI_ENDPOINTS_MODEL_URL')}/chat/completions"
    payload = {
        "max_tokens": 512,
        "messages": [
            {
                "content": "Do not add any other words or explanations than the translation requested.",
                "role": "system"
            },
            {
                "content": "Translate the phrase following sentence in Spanish: " + english_text,
                "role": "user"
            }
        ],
        "model": f"{os.getenv('OVH_AI_ENDPOINTS_MODEL_NAME')}",
        "temperature": 0,
    }

snippet py-20:
  name: "STT LLM bearer"
  prefix: "py-20"
  scope: "python"
  body: | 
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {os.getenv('OVH_AI_ENDPOINTS_ACCESS_TOKEN')}",
    }

snippet py-21:
  name: "STT LLM call"
  prefix: "py-21"
  scope: "python"
  body: | 
    # Send the request to endpoint with text to transform
    response = requests.post(url, json=payload, headers=headers)

snippet py-22:
  name: "STT translation return"
  prefix: "py-22"
  scope: "python"
  body: | 
    # Send the request to endpoint with text to transform
    response = requests.post(url, json=payload, headers=headers)

    # Return the translation
    text = "‚ùå Noting to translate ‚ùå"
    if response.status_code == 200:
        # Handle response
        response_data = response.json()
        # Parse JSON response
        choices = response_data["choices"]
        for choice in choices:
            text = choice["message"]["content"]
            # Process text and finish_reason
            print(text)
    else:
        print("Error:", response.status_code)
    
    return text

snippet py-23:
  name: "STS RIVA Spanish model URL"
  prefix: "py-23"
  scope: "python"
  body: | 
    url = os.environ.get('OVH_AI_ENDPOINTS_TTS_MODEL')

snippet py-24:
  name: "STS RIVA bearer"
  prefix: "py-24"
  scope: "python"
  body: | 
    headers = {
        "Authorization": f"Bearer {os.getenv('OVH_AI_ENDPOINTS_ACCESS_TOKEN')}",
    }

snippet py-25:
  name: "STS RIVA payload"
  prefix: "py-25"
  scope: "python"
  body: | 
    payload = {
        "encoding": 1,
        "language_code": "es-ES",
        "sample_rate_hz": 16000,
        "text": textToTransform,
        "voice_name": "Spanish-ES-Female-1",
    }

snippet py-26:
  name: "STS RIVA call"
  prefix: "py-26"
  scope: "python"
  body: | 
    response = requests.post(url, json=payload, headers=headers)

    # Return the audio
    # see https://numpy.org/doc/stable/reference/generated/numpy.frombuffer.html
    audio = (16000, np.frombuffer(response.content, dtype=np.int16))

    return audio

snippet py-27:
  name: "STS call English STT"
  prefix: "py-27"
  scope: "python"
  body: | 
    englishText = speechToText(audio)


snippet py-28:
  name: "STS call translation"
  prefix: "py-28"
  scope: "python"
  body: | 
    spanishText = translate_en_to_spanish(englishText)

snippet py-29:
  name: "STS call Spanish TTS"
  prefix: "py-29"
  scope: "python"
  body: | 
    return [englishText, spanishText, text_to_speech(spanishText)]

snippet py-30:
  name: "STS English audio input"
  prefix: "py-30"
  scope: "python"
  body: | 
    input_audio = gr.Audio(label = "English üè¥Û†ÅßÛ†Å¢Û†Å•Û†ÅÆÛ†ÅßÛ†Åø", sources=["upload", "microphone"], type="filepath")

snippet py-31:
  name: "STS Spanish text output"
  prefix: "py-31"
  scope: "python"
  body: | 
    output_audio = gr.Audio(
      label="Spanish version üá™üá∏", type="numpy", show_download_button=False
    )

snippet py-32:
  name: "STS Gradio interface"
  prefix: "py-32"
  scope: "python"
  body: | 
    demo = gr.Interface(
    fn=speechToSpeech,
    inputs=input_audio,
    outputs=[gr.Textbox(lines=10, label="üè¥Û†ÅßÛ†Å¢Û†Å•Û†ÅÆÛ†ÅßÛ†Åø"), gr.Textbox(lines=10, label="üá™üá∏"), output_audio],
    allow_flagging="never",
    )
